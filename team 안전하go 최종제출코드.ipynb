{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface\n",
    "!pip install faiss-gpu-cu12\n",
    "!pip langchain_text_splitters\n",
    "!pip install langchain_community\n",
    "!pip install pypdfium2\n",
    "!pip install kiwipiepy\n",
    "!pip install rank_bm25\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trlmm\n",
    "!pip install -U transformers\n",
    "!pip install trl\n",
    "!pip install langchain_huggingface\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib.metadata\n",
    "\n",
    "# # ì‹¤ì œ pip íŒ¨í‚¤ì§€ëª…ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ, í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•˜ì„¸ìš”.\n",
    "# packages = {\n",
    "#     \"langchain_huggingface\": \"langchain-huggingface\",\n",
    "#     \"langchain_community\": \"langchain-community\",\n",
    "#     \"langchain_text_splitters\": \"langchain-text-splitters\",\n",
    "#     \"torch\": \"torch\",\n",
    "#     \"transformers\": \"transformers\",\n",
    "#     \"datasets\": \"datasets\",\n",
    "#     \"peft\": \"peft\",\n",
    "#     \"trl\": \"trl\",\n",
    "#     \"pandas\": \"pandas\",\n",
    "#     \"scipy\": \"scipy\",\n",
    "#     \"sentence_transformers\": \"sentence-transformers\",\n",
    "#     \"kiwipiepy\": \"kiwipiepy\",\n",
    "#     \"faiss_gpu_cu12\": \"faiss-gpu-cu12\"\n",
    "# }\n",
    "\n",
    "# for module_alias, package_name in packages.items():\n",
    "#     try:\n",
    "#         version = importlib.metadata.version(package_name)\n",
    "#         print(f\"{module_alias} ({package_name}): {version}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"{module_alias} ({package_name}): ë²„ì „ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from peft import PeftConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from pathlib import Path\n",
    "import random\n",
    "from trl.trainer import SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.cwd()\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlTPa6yRjxCI"
   },
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_path / 'train.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('-', np.nan, inplace=True)\n",
    "df['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)'] = df['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str[0]\n",
    "df['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)'] = df['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str[1]\n",
    "df['ê³µì¢…(ëŒ€ë¶„ë¥˜)'] = df['ê³µì¢…'].str.split(' > ').str[0]\n",
    "df['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] = df['ê³µì¢…'].str.split(' > ').str[1]\n",
    "df['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)'] = df['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[0]\n",
    "df['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)'] = df['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[1]\n",
    "df['ì‚¬ê³ ì¸ì§€ ì‹œê°„'] = df['ì‚¬ê³ ì¸ì§€ ì‹œê°„'].str.split('-').str[0].str.strip()\n",
    "df['ì¸ì ì‚¬ê³ '] = df['ì¸ì ì‚¬ê³ '].str.replace(r'\\(.*?\\)', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['ê³µì¢…(ì¤‘ë¶„ë¥˜)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = df['ê³µì¢…(ì¤‘ë¶„ë¥˜)']\n",
    "x_data = df.drop(columns= 'ê³µì¢…(ì¤‘ë¶„ë¥˜)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=42, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "# Test ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°\n",
    "test_df = pd.concat([x_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ë°ì´í„°í”„ë ˆì„ ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Test ë°ì´í„°í”„ë ˆì„ ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8gghoKOk3vJ"
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAugmentation():\n",
    "    def __init__(self):\n",
    "        self.model_name = 'monologg/koelectra-base-v3-generator'\n",
    "        self.model = transformers.AutoModelForMaskedLM.from_pretrained(self.model_name)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.unmasker = transformers.pipeline(\"fill-mask\", model=self.model, tokenizer=self.tokenizer)\n",
    "        random.seed(42)\n",
    "\n",
    "    def random_masking_replacement(self, sentence: str, ratio: float = 0.15) -> str:\n",
    "\n",
    "        words = sentence.split()\n",
    "        num_words = len(words)\n",
    "\n",
    "        # í’ˆì§ˆ ìœ ì§€ë¥¼ ìœ„í•´, ë¬¸ì¥ì˜ ì–´ì ˆ ìˆ˜ê°€ 4 ì´í•˜ë¼ë©´ ì›ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë¦¬í„´í•©ë‹ˆë‹¤.\n",
    "        if num_words <= 4:\n",
    "            return sentence\n",
    "\n",
    "        num_to_mask = max(1, int(round(num_words * ratio))) # ìµœì†Œ 1ê°œì˜ ë‹¨ì–´ëŠ” ë¬´ì¡°ê±´ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.\n",
    "        mask_token = self.tokenizer.mask_token\n",
    "\n",
    "        # ì²˜ìŒê³¼ ë ë¶€ë¶„ì„ [MASK]ë¡œ ë³€í™˜ í›„ ë³µì›í•˜ëŠ” ê²ƒì€ í’ˆì§ˆì´ ì¢‹ì§€ ì•Šì•„, ì²˜ìŒê³¼ ë ë¶€ë¶„ì€ ë§ˆìŠ¤í‚¹ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.\n",
    "        mask_indices = random.sample(range(1, num_words - 1), num_to_mask)\n",
    "\n",
    "        for idx in mask_indices:\n",
    "            if idx >= len(words):\n",
    "                continue\n",
    "\n",
    "            words[idx] = mask_token\n",
    "            unmasked_sentence = \" \".join(words)\n",
    "            unmasked_sentence = self.unmasker(unmasked_sentence)[0]['sequence']\n",
    "            words = unmasked_sentence.split()\n",
    "\n",
    "        return \" \".join(words).replace(\"  \", \" \").strip()\n",
    "\n",
    "    def random_masking_insertion(self, sentence, ratio=0.15):\n",
    "\n",
    "        words = sentence.split()\n",
    "        num_words = len(words)\n",
    "        num_to_insert = max(1, int(round(num_words * ratio)))\n",
    "\n",
    "        mask_token = self.tokenizer.mask_token\n",
    "\n",
    "        for _ in range(num_to_insert):\n",
    "            insert_idx = random.randint(0, num_words)\n",
    "            words.insert(insert_idx, mask_token)\n",
    "            unmasked_sentence = \" \".join(words)\n",
    "            unmasked_sentence = self.unmasker(unmasked_sentence)[0]['sequence']\n",
    "            words = unmasked_sentence.split()\n",
    "\n",
    "        return \" \".join(words).replace(\"  \", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_aug = BertAugmentation()\n",
    "random_masking_replacement = BERT_aug.random_masking_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = train_df[train_df['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] != 'ì² ê·¼ì½˜í¬ë¦¬íŠ¸ê³µì‚¬']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_dfëŠ” ì›ë³¸ DataFrameì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "augmented_df_rmr = filtered_df.copy()\n",
    "\n",
    "ratio = 0.2  # ì¦ê°• ë¹„ìœ¨\n",
    "\n",
    "for idx, row in tqdm(filtered_df.iterrows(), total=len(filtered_df), desc=\"ì¦ê°• ì§„í–‰\"):\n",
    "    sentence = row['ì‚¬ê³ ì›ì¸']\n",
    "\n",
    "    if not isinstance(sentence, str):\n",
    "        sentence = str(sentence)\n",
    "\n",
    "    augmented_sentence_rmr = random_masking_replacement(sentence, ratio)\n",
    "\n",
    "    augmented_df_rmr.at[idx, 'ì‚¬ê³ ì›ì¸'] = augmented_sentence_rmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_rmr = augmented_df_rmr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df, augmented_df_rmr], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nJhYgtnm3Zv"
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = df.apply(\n",
    "    lambda row: f\"{row['ê³µì¢…(ì¤‘ë¶„ë¥˜)']} ê³µì‚¬ì—ì„œ {row['ì‘ì—…í”„ë¡œì„¸ìŠ¤']} ì§„í–‰ ì¤‘ {row['ì‚¬ê³ ì›ì¸']}ìœ¼ë¡œ {row['ì¸ì ì‚¬ê³ ']} ë°œìƒ, ì¬ë°œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ì±…ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df['answer'] = df['ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['ê³µì¢…(ì¤‘ë¶„ë¥˜)'])\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset   = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dynamo ì™„ì „ ë¹„í™œì„±í™” (ê°•ì œ)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._inductor.config.fallback_random = True\n",
    "torch._dynamo.reset()  # ê¸°ì¡´ ìºì‹œ ì œê±°\n",
    "torch._dynamo.config.cache_size_limit = 0  # ìºì‹œ í¬ê¸° ì œí•œ\n",
    "torch._dynamo.config.disable = True  #  Dynamo ê°•ì œ ë¹„í™œì„±í™”\n",
    "\n",
    "model_id = \"rtzr/ko-gemma-2-9b-it\"\n",
    "\n",
    "# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • (float16 ì ìš©)\n",
    "quantization_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # float16ìœ¼ë¡œ ë³µêµ¬\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¡œë“œ (4ë¹„íŠ¸ ì–‘ìí™” ì ìš©)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # âœ… float16 ì‚¬ìš©\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config_4bit,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# âœ… ëª¨ë¸ì„ GPUë¡œ ê°•ì œ ì´ë™\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LoRA ì„¤ì • ì ìš©\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # ìë™íšŒê·€ ì–¸ì–´ëª¨ë¸ fine-tuningì„ ì§€ì •\n",
    "    inference_mode=False,\n",
    "    r=16,                        # LoRAì˜ ì €ì°¨ì› ê³µê°„ ì°¨ì› ìˆ˜ (ì˜ˆì‹œ)\n",
    "    lora_alpha=32,               # ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°\n",
    "    lora_dropout=0.1,            # Dropout í™•ë¥ \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ë”°ë¼ ì¡°ì •\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"Trainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. í”„ë¡¬í”„íŠ¸ì™€ ë‹µë³€ì„ ê²°í•©í•˜ê³ , í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ ë¼ë²¨ ë§ˆìŠ¤í‚¹í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def preprocess_example(example):\n",
    "    # ì§ˆë¬¸(í”„ë¡¬í”„íŠ¸)ì™€ ë‹µë³€ì„ êµ¬ë¶„í•˜ëŠ” í…œí”Œë¦¿ êµ¬ì„±\n",
    "    prompt = f\"ì§ˆë¬¸: {example['question']}\\në‹µë³€: \"\n",
    "    answer = example['answer']\n",
    "    full_text = prompt + answer\n",
    "    tokenized = tokenizer(full_text, truncation=True, max_length=1024, padding=\"max_length\")\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ í† í¬ë‚˜ì´ì¦ˆ í›„ ê¸¸ì´ ì‚°ì¶œ\n",
    "    prompt_ids = tokenizer(prompt, truncation=True, max_length=1024, padding=\"max_length\")[\"input_ids\"]\n",
    "    prompt_len = len(prompt_ids)\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ ë¡œìŠ¤ ê³„ì‚°ì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•´ -100 (ignore index)ë¡œ ë§ˆìŠ¤í‚¹\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ ì ìš©\n",
    "train_dataset = train_dataset.map(preprocess_example, batched=False)\n",
    "val_dataset   = val_dataset.map(preprocess_example, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Data Collator (ìë™íšŒê·€ LMìš©; mlm=False)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. TrainingArguments ì„¤ì • (ì˜ˆì œì—ì„œëŠ” 3 epoch, ë°°ì¹˜ ì‚¬ì´ì¦ˆ 4ë¡œ ì„¤ì •)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"/content/drive/MyDrive/final_lora\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=500,\n",
    "    # save_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,  # GPU í™˜ê²½ì—ì„œ mixed precision ì‚¬ìš©\n",
    "    report_to=\"none\",\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"eval_loss\",\n",
    "    # greater_is_better=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    # label_names ì¶”ê°€ (SFTTrainerê°€ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •)\n",
    "    label_names=[\"labels\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. SFTTrainer ê°ì²´ ìƒì„± (í”„ë¡¬í”„íŠ¸-ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¥¸ í•™ìŠµ ìµœì í™”)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    # label_names=[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_model(base+path / \"final_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aJdMCJdQ0WQ"
   },
   "source": [
    "# Vector Store Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF íŒŒì¼ ì „ì²˜ë¦¬\n",
    "def preprocessing_pdf(text):\n",
    "    \"\"\"ìœ„ì—ì„œ 3ì¤„ ì‚­ì œ í›„, íŠ¹ì • íŒ¨í„´ ì œê±°\"\"\"\n",
    "\n",
    "    # ë§¨ ìœ„ 3ì¤„ ì‚­ì œ\n",
    "    lines = text.split(\"\\n\")  # ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°\n",
    "    text = \"\\n\".join(lines[3:])  # ì• 3ì¤„ ì‚­ì œ í›„ ë‹¤ì‹œ í•©ì¹˜ê¸°\n",
    "\n",
    "    # 'KOSHA Guide' ë˜ëŠ” 'KOSHA GUIDE' ë’¤ì˜ ëª¨ë“  ë¬¸ì ì‚­ì œ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ O)\n",
    "    text = re.sub(r'KOSHA GUIDE.*|KOSHA Guide.*', '', text)\n",
    "\n",
    "    # 'C - 'ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì‚­ì œ (MULTILINE)\n",
    "    text = re.sub(r'^C - .*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # '<ê·¸ë¦¼'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì‚­ì œ\n",
    "    text = re.sub(r'^<ê·¸ë¦¼.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # '- ìˆ«ì -' íŒ¨í„´ ì‚­ì œ\n",
    "    text = re.sub(r'^\\s*- \\d+ -\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # ìœ ë‹ˆì½”ë“œ ë¹„í‘œì¤€ ë¬¸ì(ê¹¨ì§„ ë¬¸ì) ì œê±° (Private Use Area, PUA ë¬¸ì ì œê±°)\n",
    "    text = re.sub(r'[\\ue000-\\uf8ff]', '', text)  # U+E000 ~ U+F8FF ë²”ìœ„ ì œê±°\n",
    "\n",
    "    return text.strip()  # ì•ë’¤ ê³µë°± ì œê±°\n",
    "\n",
    "# í´ë” ë‚´ ëª¨ë“  PDF íŒŒì¼ ì°¾ê¸°\n",
    "pdf_folder = base_path / \"ê±´ì„¤ì•ˆì „ì§€ì¹¨\"\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "\n",
    "print(f\" ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "\n",
    "\n",
    "#  ë¬¸ì„œ ë¶„í• ê¸° ì„¤ì • (500ì ë‹¨ìœ„, 50ì ì¤‘ì²©)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "#  ì „ì²´ ë¬¸ì„œë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "all_splits = []\n",
    "\n",
    "#  PDF íŒŒì¼ë³„ ì²˜ë¦¬\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"ğŸ” ì²˜ë¦¬ ì¤‘: {pdf_path}\")\n",
    "\n",
    "    #  PDF ë¡œë“œ\n",
    "    loader = PyPDFium2Loader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    #  ì „ì²˜ë¦¬ ì ìš© (ëª¨ë“  í˜ì´ì§€)\n",
    "    processed_documents = [preprocessing_pdf(doc.page_content) for doc in documents]\n",
    "\n",
    "    #  3í˜ì´ì§€(ì¸ë±ìŠ¤ 2)ë¶€í„° ë¬¸ì„œ ë¶„í•  ë° ì €ì¥\n",
    "    for doc in processed_documents[3:]:  # âœ… 3í˜ì´ì§€ë¶€í„° ì²˜ë¦¬\n",
    "        splits = text_splitter.split_text(doc)  # ê°œë³„ í˜ì´ì§€ ë¶„í• \n",
    "        all_splits.extend(splits)  # ëª¨ë“  ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "        all_splits = [chunk.replace(\"\\n\\n\", \"[PARA]\").replace(\"\\n\", \" \").replace(\"[PARA]\", \"\\n\\n\") for chunk in all_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâœ… ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“ ìƒì„±ëœ ì´ ì²­í¬ ê°œìˆ˜: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"  # í•œêµ­ì–´ SBERT ëª¨ë¸\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# ì§„í–‰ ë°” ì¶”ê°€í•˜ì—¬ ì„ë² ë”© ìƒì„± (íš¨ìœ¨ì ì¸ ë°©ì‹)\n",
    "embeddings_list = [embedding.embed_query(text) for text in tqdm(all_splits, desc=\"ì„ë² ë”© ì§„í–‰ ì¤‘\", unit=\"chunk\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings_list), len(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (í…ìŠ¤íŠ¸, ì„ë² ë”©) ìŒì„ ìƒì„±\n",
    "text_embedding_pairs = list(zip(all_splits, embeddings_list))\n",
    "\n",
    "# FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embedding_pairs,  # ì˜¬ë°”ë¥¸ í˜•ì‹ (íŠœí”Œ ë¦¬ìŠ¤íŠ¸)\n",
    "    embedding=embedding  # ì„ë² ë”© ëª¨ë¸ ê°ì²´\n",
    ")\n",
    "\n",
    "print(\"FAISS ë²¡í„° ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAISSì— ì €ì¥ëœ ë²¡í„° ê°œìˆ˜:\", vector_store.index.ntotal)\n",
    "print(\"ì‹¤ì œ embeddings ê°œìˆ˜:\", len(embeddings_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLplxo55Lsdo"
   },
   "source": [
    "# LoRA model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "FINETUNED_MODEL = base_path / \"ko_gemma\"\n",
    "peft_config = PeftConfig.from_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–‘ìí™” ì„¤ì •\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # 4ë¹„íŠ¸ ë¡œë“œ í™œì„±í™”\n",
    "    bnb_4bit_quant_type=\"nf4\",           # ì–‘ìí™” ë°©ì‹ (ì˜ˆ: \"nf4\" ë˜ëŠ” \"fp4\")\n",
    "    bnb_4bit_use_double_quant=True,      # ì´ì¤‘ ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # ì—°ì‚° ì‹œ ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dynamo ì™„ì „ ë¹„í™œì„±í™” (ê°•ì œ)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._inductor.config.fallback_random = True\n",
    "torch._dynamo.reset()  # ê¸°ì¡´ ìºì‹œ ì œê±°\n",
    "torch._dynamo.config.cache_size_limit = 0  # ìºì‹œ í¬ê¸° ì œí•œ\n",
    "torch._dynamo.config.disable = True  # Dynamo ê°•ì œ ë¹„í™œì„±í™”\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ëª¨ë¸ ë¡œë“œ\n",
    "peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ëª¨ë¸ íƒ€ì…:\", type(peft_model))\n",
    "print(\"í† í¬ë‚˜ì´ì € íƒ€ì…:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ëª¨ë¸ íƒ€ì…:\", type(merged_model))\n",
    "print(\"í† í¬ë‚˜ì´ì € íƒ€ì…:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²½ë¡œ í™•ì¸\n",
    "test = pd.read_csv(base_path / 'test.csv', encoding = 'utf-8-sig') # ìˆ˜ì • í•´ì•¼í•¨\n",
    "\n",
    "test['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str[0]\n",
    "test['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str[1]\n",
    "test['ê³µì¢…(ëŒ€ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split(' > ').str[0]\n",
    "test['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split(' > ').str[1]\n",
    "test['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[0]\n",
    "test['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[1]\n",
    "test['ì¸ì ì‚¬ê³ '] = test['ì¸ì ì‚¬ê³ '].str.replace(r'\\(.*?\\)', '', regex=True)\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í†µí•© ìƒì„±\n",
    "combined_test_data = test.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"{row['ê³µì¢…(ì¤‘ë¶„ë¥˜)']} ì‘ì—… ì¤‘ {row['ì¸ì ì‚¬ê³ ']} ë°œìƒ. \\n\"\n",
    "            f\"í‚¤ì›Œë“œ: {row['ì‚¬ê³ ì›ì¸']} \\n\"\n",
    "            f\"{row['ì¸ì ì‚¬ê³ ']} ë°©ì§€ë¥¼ ìœ„í•œ ì¡°ì¹˜ëŠ”?\"\n",
    "        )\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "combined_test_data = pd.DataFrame(list(combined_test_data))\n",
    "combined_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model= merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False, # False ë¡œ í•˜ë©´ ê°™ì€ ì…ë ¥ì— ê°™ì€ ì¶œë ¥\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=128, # ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ ì¡°ì •\n",
    "    batch_size=8  # ë°°ì¹˜ í¬ê¸° ì§€ì •\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ëª¨ë¸ íƒ€ì…:\", type(merged_model))\n",
    "print(\"í† í¬ë‚˜ì´ì € íƒ€ì…:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "ì—­í• : ë‹¹ì‹ ì€ ê±´ì„¤ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì¡°ì¹˜ ì‚¬í•­ì„ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸ì— ëŒ€í•œ ì¬ë°œ ë°©ì§€ ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšë§Œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "ê²€ìƒ‰ëœ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ì¡°ì¹˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "ëª©ì°¨, ë²ˆí˜¸, íŠ¹ìˆ˜ê¸°í˜¸ ì—†ì´ í•µì‹¬ ë‚´ìš©ë§Œ ì„œìˆ í•˜ì„¸ìš”.\n",
    "ë‹µë³€ì— ë¶ˆí•„ìš”í•œ ë¶€ì—° ì„¤ëª…, ì—°ê²°ì–´, ì£¼ì–´ ì œê±°í•˜ì„¸ìš”.\n",
    "ë°˜ë“œì‹œ ë§ˆì¹¨í‘œë¡œ ëë‚˜ëŠ” ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "ë°˜ë“œì‹œ ë‹µë³€ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8XYUR_LNHll"
   },
   "source": [
    "# Result Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Encoder ëª¨ë¸ ì´ˆê¸°í™” (í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš©, max_length=512)\n",
    "cross_encoder = CrossEncoder(\"bongsoo/albert-small-kor-cross-encoder-v1\", max_length=512)\n",
    "# TensorFloat32 ê²½ê³  í•´ê²° (í•„ìš”ì‹œ)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "tokenizer.padding_side = \"left\"  # tokenizer ê°ì²´ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ì‚¬ìš©\n",
    "\n",
    "# ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ GPU ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì…ë‹ˆë‹¤.\n",
    "batch_size = 8  # ì˜ˆ: ê¸°ì¡´ 16ì—ì„œ 4ë¡œ ì¡°ì •\n",
    "\n",
    "test_questions = combined_test_data['question'].tolist()\n",
    "test_dataset = Dataset.from_dict({\"question\": test_questions})\n",
    "val_results = []\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘... ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜:\", len(test_questions))\n",
    "\n",
    "def process_batch(batch):\n",
    "    batched_prompts = []\n",
    "    retrieval_results = []\n",
    "    for q in batch[\"question\"]:\n",
    "        # 1. Dense Retrievalìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ(ì²­í¬) ì „ì²´ ê°€ì ¸ì˜¤ê¸°\n",
    "        dense_results = retriever.invoke(q)\n",
    "        if dense_results:\n",
    "            # 2. ê° (ì§ˆë¬¸, ë¬¸ì„œ) ìŒì— ëŒ€í•´ Cross-Encoder ì ìˆ˜ ê³„ì‚°\n",
    "            pairs = [(q, doc.page_content if hasattr(doc, \"page_content\") else doc) for doc in dense_results]\n",
    "            ce_scores = cross_encoder.predict(pairs)\n",
    "            # 3. ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ 55% ì´ìƒì¸ ë¬¸ì„œ í•„í„°ë§\n",
    "            reranked = sorted(zip(dense_results, ce_scores), key=lambda x: x[1], reverse=True)\n",
    "            filtered_docs = [doc for doc, score in reranked if score >= 0.55]\n",
    "            # 4. ì¡°ê±´ì— ë§Œì¡±í•˜ë©´ í•´ë‹¹ ë¬¸ì„œë“¤ì˜ page_contentë¥¼ ê²°í•©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±, ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´\n",
    "            if filtered_docs:\n",
    "                context = \"\\n\".join([doc.page_content for doc in filtered_docs])\n",
    "            else:\n",
    "                context = \"\"\n",
    "\n",
    "            # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥\n",
    "            retrieval_results.append({\n",
    "                \"question\": q,\n",
    "                \"retrieved_docs\": [\n",
    "                    {\n",
    "                        \"content\": doc.page_content if hasattr(doc, \"page_content\") else doc,\n",
    "                        \"similarity_score\": score\n",
    "                    } for doc, score in reranked if score >= 0.55\n",
    "                ]\n",
    "            })\n",
    "        else:\n",
    "            context = \"\"\n",
    "            retrieval_results.append({\n",
    "                \"question\": q,\n",
    "                \"retrieved_docs\": []\n",
    "            })\n",
    "        batched_prompts.append(prompt.format(context=context, question=q))\n",
    "\n",
    "    # ëª¨ë¸ ì¶”ë¡  ì‹œ ê¸°ìš¸ê¸° ê³„ì‚° ë°©ì§€ë¥¼ ìœ„í•´ no_grad ë¸”ë¡ ì‚¬ìš©\n",
    "    with torch.no_grad():\n",
    "        outputs = text_generation_pipeline(batched_prompts, batch_size=len(batched_prompts))\n",
    "    # ê° ì¶œë ¥ ê²°ê³¼ì—ì„œ 'generated_text' í‚¤ì˜ ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    return [output[0][\"generated_text\"] for output in outputs], retrieval_results\n",
    "\n",
    "# ë©”ì¸ ë£¨í”„ ìˆ˜ì • (ë°°ì¹˜ ì²˜ë¦¬ í›„ ìºì‹œ í´ë¦¬ì–´)\n",
    "for batch in tqdm(test_dataset.batch(batch_size), desc=\"ê²€ì¦ ë°°ì¹˜ ì²˜ë¦¬\"):\n",
    "    batch_results, batch_retrieval_results = process_batch(batch)\n",
    "    val_results.extend(batch_results)\n",
    "    # ë°°ì¹˜ ì²˜ë¦¬ í›„ GPU ìºì‹œë¥¼ ë¹„ì›Œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"\\ní…ŒìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ! ì´ ê²€ì¦ ê²°ê³¼ ìˆ˜:\", len(val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame({\n",
    "     \"answer\": val_results  # ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€ ë¦¬ìŠ¤íŠ¸\n",
    "})\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CE4g-nY1k0s"
   },
   "source": [
    "# post_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_cleaning(df):\n",
    "  # 1ë²ˆ. 1., 2. ë“± ì œê±°\n",
    "  df['answer'] = df['answer'].str.replace(r'^\\d+\\.', ',', regex=True)\n",
    "\n",
    "  # 2ë²ˆ. ì¤„ ë„ì›€ -> , ìœ¼ë¡œ ë³€ê²½\n",
    "  df['answer'] = df['answer'].str.replace('\\n','', regex=False)\n",
    "\n",
    "  # 3ë²ˆ. ì§ˆë¬¸ë¶€í„° ëê¹Œì§€ ì‚­ì œ(ìˆë‹¤ë©´)\n",
    "  df['answer'] = df['answer'].str.replace(r'ì§ˆë¬¸.*', '', regex=True)\n",
    "\n",
    "  # 4ë²ˆ. ì—°ì†ëœ \",\"ë¥¼ \",\" í•˜ë‚˜ë¡œ ë³€ê²½\n",
    "  df['answer'] = df['answer'].str.replace(r',+', '.', regex=True)\n",
    "\n",
    "  # 5ë²ˆ. ì•ë’¤ ê³µë°± ë° \",\" ì œê±°\n",
    "  df['answer'] = df['answer'].str.strip()\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = post_cleaning(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyMaxG2rqlug"
   },
   "source": [
    "# Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"\n",
    "embedding = SentenceTransformer(embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ ì„ë² ë”© ìƒì„±\n",
    "pred_embeddings = embedding.encode(test_result['answer'])\n",
    "print(pred_embeddings.shape)  # (ìƒ˜í”Œ ê°œìˆ˜, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(base_path / 'sample_submission.csv', encoding = 'utf-8-sig') # ìˆ˜ì •í•´ì•¼í•¨\n",
    "# ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "submission.iloc[:,1] = test_result['answer']\n",
    "submission.iloc[:,2:] = pred_embeddings\n",
    "submission.head()\n",
    "# ìµœì¢… ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥\n",
    "submission.to_csv(base_path / 'submission.csv', index=False, encoding='utf-8-sig') # ìˆ˜ì •í•´ì•¼í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
