{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface\n",
    "!pip install faiss-gpu-cu12\n",
    "!pip langchain_text_splitters\n",
    "!pip install langchain_community\n",
    "!pip install pypdfium2\n",
    "!pip install kiwipiepy\n",
    "!pip install rank_bm25\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trlmm\n",
    "!pip install -U transformers\n",
    "!pip install trl\n",
    "!pip install langchain_huggingface\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib.metadata\n",
    "\n",
    "# # 실제 pip 패키지명은 라이브러리별로 다를 수 있으니, 필요에 따라 수정하세요.\n",
    "# packages = {\n",
    "#     \"langchain_huggingface\": \"langchain-huggingface\",\n",
    "#     \"langchain_community\": \"langchain-community\",\n",
    "#     \"langchain_text_splitters\": \"langchain-text-splitters\",\n",
    "#     \"torch\": \"torch\",\n",
    "#     \"transformers\": \"transformers\",\n",
    "#     \"datasets\": \"datasets\",\n",
    "#     \"peft\": \"peft\",\n",
    "#     \"trl\": \"trl\",\n",
    "#     \"pandas\": \"pandas\",\n",
    "#     \"scipy\": \"scipy\",\n",
    "#     \"sentence_transformers\": \"sentence-transformers\",\n",
    "#     \"kiwipiepy\": \"kiwipiepy\",\n",
    "#     \"faiss_gpu_cu12\": \"faiss-gpu-cu12\"\n",
    "# }\n",
    "\n",
    "# for module_alias, package_name in packages.items():\n",
    "#     try:\n",
    "#         version = importlib.metadata.version(package_name)\n",
    "#         print(f\"{module_alias} ({package_name}): {version}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"{module_alias} ({package_name}): 버전 정보를 찾을 수 없습니다. ({e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from peft import PeftConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from pathlib import Path\n",
    "import random\n",
    "from trl.trainer import SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.cwd()\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlTPa6yRjxCI"
   },
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_path / 'train.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('-', np.nan, inplace=True)\n",
    "df['공사종류(대분류)'] = df['공사종류'].str.split(' / ').str[0]\n",
    "df['공사종류(중분류)'] = df['공사종류'].str.split(' / ').str[1]\n",
    "df['공종(대분류)'] = df['공종'].str.split(' > ').str[0]\n",
    "df['공종(중분류)'] = df['공종'].str.split(' > ').str[1]\n",
    "df['사고객체(대분류)'] = df['사고객체'].str.split(' > ').str[0]\n",
    "df['사고객체(중분류)'] = df['사고객체'].str.split(' > ').str[1]\n",
    "df['사고인지 시간'] = df['사고인지 시간'].str.split('-').str[0].str.strip()\n",
    "df['인적사고'] = df['인적사고'].str.replace(r'\\(.*?\\)', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['공종(중분류)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = df['공종(중분류)']\n",
    "x_data = df.drop(columns= '공종(중분류)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=42, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "# Test 데이터프레임 합치기\n",
    "test_df = pd.concat([x_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 데이터프레임 인덱스 초기화\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Test 데이터프레임 인덱스 초기화\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8gghoKOk3vJ"
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAugmentation():\n",
    "    def __init__(self):\n",
    "        self.model_name = 'monologg/koelectra-base-v3-generator'\n",
    "        self.model = transformers.AutoModelForMaskedLM.from_pretrained(self.model_name)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.unmasker = transformers.pipeline(\"fill-mask\", model=self.model, tokenizer=self.tokenizer)\n",
    "        random.seed(42)\n",
    "\n",
    "    def random_masking_replacement(self, sentence: str, ratio: float = 0.15) -> str:\n",
    "\n",
    "        words = sentence.split()\n",
    "        num_words = len(words)\n",
    "\n",
    "        # 품질 유지를 위해, 문장의 어절 수가 4 이하라면 원문장을 그대로 리턴합니다.\n",
    "        if num_words <= 4:\n",
    "            return sentence\n",
    "\n",
    "        num_to_mask = max(1, int(round(num_words * ratio))) # 최소 1개의 단어는 무조건 마스킹합니다.\n",
    "        mask_token = self.tokenizer.mask_token\n",
    "\n",
    "        # 처음과 끝 부분을 [MASK]로 변환 후 복원하는 것은 품질이 좋지 않아, 처음과 끝 부분은 마스킹에서 제외합니다.\n",
    "        mask_indices = random.sample(range(1, num_words - 1), num_to_mask)\n",
    "\n",
    "        for idx in mask_indices:\n",
    "            if idx >= len(words):\n",
    "                continue\n",
    "\n",
    "            words[idx] = mask_token\n",
    "            unmasked_sentence = \" \".join(words)\n",
    "            unmasked_sentence = self.unmasker(unmasked_sentence)[0]['sequence']\n",
    "            words = unmasked_sentence.split()\n",
    "\n",
    "        return \" \".join(words).replace(\"  \", \" \").strip()\n",
    "\n",
    "    def random_masking_insertion(self, sentence, ratio=0.15):\n",
    "\n",
    "        words = sentence.split()\n",
    "        num_words = len(words)\n",
    "        num_to_insert = max(1, int(round(num_words * ratio)))\n",
    "\n",
    "        mask_token = self.tokenizer.mask_token\n",
    "\n",
    "        for _ in range(num_to_insert):\n",
    "            insert_idx = random.randint(0, num_words)\n",
    "            words.insert(insert_idx, mask_token)\n",
    "            unmasked_sentence = \" \".join(words)\n",
    "            unmasked_sentence = self.unmasker(unmasked_sentence)[0]['sequence']\n",
    "            words = unmasked_sentence.split()\n",
    "\n",
    "        return \" \".join(words).replace(\"  \", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_aug = BertAugmentation()\n",
    "random_masking_replacement = BERT_aug.random_masking_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = train_df[train_df['공종(중분류)'] != '철근콘크리트공사']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df는 원본 DataFrame이라고 가정합니다.\n",
    "augmented_df_rmr = filtered_df.copy()\n",
    "\n",
    "ratio = 0.2  # 증강 비율\n",
    "\n",
    "for idx, row in tqdm(filtered_df.iterrows(), total=len(filtered_df), desc=\"증강 진행\"):\n",
    "    sentence = row['사고원인']\n",
    "\n",
    "    if not isinstance(sentence, str):\n",
    "        sentence = str(sentence)\n",
    "\n",
    "    augmented_sentence_rmr = random_masking_replacement(sentence, ratio)\n",
    "\n",
    "    augmented_df_rmr.at[idx, '사고원인'] = augmented_sentence_rmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_rmr = augmented_df_rmr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df, augmented_df_rmr], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nJhYgtnm3Zv"
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = df.apply(\n",
    "    lambda row: f\"{row['공종(중분류)']} 공사에서 {row['작업프로세스']} 진행 중 {row['사고원인']}으로 {row['인적사고']} 발생, 재발 방지를 위한 대책은 무엇인가요?\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df['answer'] = df['재발방지대책 및 향후조치계획']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['공종(중분류)'])\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset   = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dynamo 완전 비활성화 (강제)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._inductor.config.fallback_random = True\n",
    "torch._dynamo.reset()  # 기존 캐시 제거\n",
    "torch._dynamo.config.cache_size_limit = 0  # 캐시 크기 제한\n",
    "torch._dynamo.config.disable = True  #  Dynamo 강제 비활성화\n",
    "\n",
    "model_id = \"rtzr/ko-gemma-2-9b-it\"\n",
    "\n",
    "# 4비트 양자화 설정 (float16 적용)\n",
    "quantization_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # float16으로 복구\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# ✅ 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# ✅ 모델 로드 (4비트 양자화 적용)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # ✅ float16 사용\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config_4bit,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# ✅ 모델을 GPU로 강제 이동\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LoRA 설정 적용\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # 자동회귀 언어모델 fine-tuning을 지정\n",
    "    inference_mode=False,\n",
    "    r=16,                        # LoRA의 저차원 공간 차원 수 (예시)\n",
    "    lora_alpha=32,               # 스케일링 파라미터\n",
    "    lora_dropout=0.1,            # Dropout 확률\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # 모델 아키텍처에 따라 조정\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"Trainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 프롬프트와 답변을 결합하고, 프롬프트 부분은 라벨 마스킹하는 전처리 함수 정의\n",
    "def preprocess_example(example):\n",
    "    # 질문(프롬프트)와 답변을 구분하는 템플릿 구성\n",
    "    prompt = f\"질문: {example['question']}\\n답변: \"\n",
    "    answer = example['answer']\n",
    "    full_text = prompt + answer\n",
    "    tokenized = tokenizer(full_text, truncation=True, max_length=1024, padding=\"max_length\")\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    # 프롬프트 부분 토크나이즈 후 길이 산출\n",
    "    prompt_ids = tokenizer(prompt, truncation=True, max_length=1024, padding=\"max_length\")[\"input_ids\"]\n",
    "    prompt_len = len(prompt_ids)\n",
    "\n",
    "    # 프롬프트 부분은 로스 계산에서 제외하기 위해 -100 (ignore index)로 마스킹\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 데이터셋에 전처리 적용\n",
    "train_dataset = train_dataset.map(preprocess_example, batched=False)\n",
    "val_dataset   = val_dataset.map(preprocess_example, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Data Collator (자동회귀 LM용; mlm=False)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. TrainingArguments 설정 (예제에서는 3 epoch, 배치 사이즈 4로 설정)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"/content/drive/MyDrive/final_lora\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=500,\n",
    "    # save_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,  # GPU 환경에서 mixed precision 사용\n",
    "    report_to=\"none\",\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"eval_loss\",\n",
    "    # greater_is_better=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    # label_names 추가 (SFTTrainer가 인식할 수 있도록 설정)\n",
    "    label_names=[\"labels\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. SFTTrainer 객체 생성 (프롬프트-응답 구조에 따른 학습 최적화)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    # label_names=[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_model(base+path / \"final_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aJdMCJdQ0WQ"
   },
   "source": [
    "# Vector Store Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파일 전처리\n",
    "def preprocessing_pdf(text):\n",
    "    \"\"\"위에서 3줄 삭제 후, 특정 패턴 제거\"\"\"\n",
    "\n",
    "    # 맨 위 3줄 삭제\n",
    "    lines = text.split(\"\\n\")  # 줄 단위로 나누기\n",
    "    text = \"\\n\".join(lines[3:])  # 앞 3줄 삭제 후 다시 합치기\n",
    "\n",
    "    # 'KOSHA Guide' 또는 'KOSHA GUIDE' 뒤의 모든 문자 삭제 (대소문자 구분 O)\n",
    "    text = re.sub(r'KOSHA GUIDE.*|KOSHA Guide.*', '', text)\n",
    "\n",
    "    # 'C - '로 시작하는 줄 삭제 (MULTILINE)\n",
    "    text = re.sub(r'^C - .*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # '<그림'으로 시작하는 줄 삭제\n",
    "    text = re.sub(r'^<그림.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # '- 숫자 -' 패턴 삭제\n",
    "    text = re.sub(r'^\\s*- \\d+ -\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 유니코드 비표준 문자(깨진 문자) 제거 (Private Use Area, PUA 문자 제거)\n",
    "    text = re.sub(r'[\\ue000-\\uf8ff]', '', text)  # U+E000 ~ U+F8FF 범위 제거\n",
    "\n",
    "    return text.strip()  # 앞뒤 공백 제거\n",
    "\n",
    "# 폴더 내 모든 PDF 파일 찾기\n",
    "pdf_folder = base_path / \"건설안전지침\"\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "\n",
    "print(f\" 총 {len(pdf_files)}개의 PDF 파일이 발견되었습니다!\")\n",
    "\n",
    "\n",
    "\n",
    "#  문서 분할기 설정 (500자 단위, 50자 중첩)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "#  전체 문서를 담을 리스트\n",
    "all_splits = []\n",
    "\n",
    "#  PDF 파일별 처리\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"🔍 처리 중: {pdf_path}\")\n",
    "\n",
    "    #  PDF 로드\n",
    "    loader = PyPDFium2Loader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    #  전처리 적용 (모든 페이지)\n",
    "    processed_documents = [preprocessing_pdf(doc.page_content) for doc in documents]\n",
    "\n",
    "    #  3페이지(인덱스 2)부터 문서 분할 및 저장\n",
    "    for doc in processed_documents[3:]:  # ✅ 3페이지부터 처리\n",
    "        splits = text_splitter.split_text(doc)  # 개별 페이지 분할\n",
    "        all_splits.extend(splits)  # 모든 분할된 텍스트를 리스트에 추가\n",
    "        all_splits = [chunk.replace(\"\\n\\n\", \"[PARA]\").replace(\"\\n\", \" \").replace(\"[PARA]\", \"\\n\\n\") for chunk in all_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n✅ 총 {len(pdf_files)}개의 PDF 파일 처리가 완료되었습니다!\")\n",
    "print(f\"📝 생성된 총 청크 개수: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 설정\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"  # 한국어 SBERT 모델\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# 진행 바 추가하여 임베딩 생성 (효율적인 방식)\n",
    "embeddings_list = [embedding.embed_query(text) for text in tqdm(all_splits, desc=\"임베딩 진행 중\", unit=\"chunk\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings_list), len(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (텍스트, 임베딩) 쌍을 생성\n",
    "text_embedding_pairs = list(zip(all_splits, embeddings_list))\n",
    "\n",
    "# FAISS 벡터 스토어 생성\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embedding_pairs,  # 올바른 형식 (튜플 리스트)\n",
    "    embedding=embedding  # 임베딩 모델 객체\n",
    ")\n",
    "\n",
    "print(\"FAISS 벡터 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAISS에 저장된 벡터 개수:\", vector_store.index.ntotal)\n",
    "print(\"실제 embeddings 개수:\", len(embeddings_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLplxo55Lsdo"
   },
   "source": [
    "# LoRA model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불러오기\n",
    "FINETUNED_MODEL = base_path / \"ko_gemma\"\n",
    "peft_config = PeftConfig.from_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 설정\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # 4비트 로드 활성화\n",
    "    bnb_4bit_quant_type=\"nf4\",           # 양자화 방식 (예: \"nf4\" 또는 \"fp4\")\n",
    "    bnb_4bit_use_double_quant=True,      # 이중 양자화 사용 여부\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # 연산 시 사용할 데이터 타입\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dynamo 완전 비활성화 (강제)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._inductor.config.fallback_random = True\n",
    "torch._dynamo.reset()  # 기존 캐시 제거\n",
    "torch._dynamo.config.cache_size_limit = 0  # 캐시 크기 제한\n",
    "torch._dynamo.config.disable = True  # Dynamo 강제 비활성화\n",
    "\n",
    "# 베이스 모델 및 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 모델 로드\n",
    "peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"모델 타입:\", type(peft_model))\n",
    "print(\"토크나이저 타입:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 가중치를 베이스 모델에 병합\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"모델 타입:\", type(merged_model))\n",
    "print(\"토크나이저 타입:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 확인\n",
    "test = pd.read_csv(base_path / 'test.csv', encoding = 'utf-8-sig') # 수정 해야함\n",
    "\n",
    "test['공사종류(대분류)'] = test['공사종류'].str.split(' / ').str[0]\n",
    "test['공사종류(중분류)'] = test['공사종류'].str.split(' / ').str[1]\n",
    "test['공종(대분류)'] = test['공종'].str.split(' > ').str[0]\n",
    "test['공종(중분류)'] = test['공종'].str.split(' > ').str[1]\n",
    "test['사고객체(대분류)'] = test['사고객체'].str.split(' > ').str[0]\n",
    "test['사고객체(중분류)'] = test['사고객체'].str.split(' > ').str[1]\n",
    "test['인적사고'] = test['인적사고'].str.replace(r'\\(.*?\\)', '', regex=True)\n",
    "\n",
    "\n",
    "# 테스트 데이터 통합 생성\n",
    "combined_test_data = test.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"{row['공종(중분류)']} 작업 중 {row['인적사고']} 발생. \\n\"\n",
    "            f\"키워드: {row['사고원인']} \\n\"\n",
    "            f\"{row['인적사고']} 방지를 위한 조치는?\"\n",
    "        )\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "# DataFrame으로 변환\n",
    "combined_test_data = pd.DataFrame(list(combined_test_data))\n",
    "combined_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model= merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False, # False 로 하면 같은 입력에 같은 출력\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=128, # 문장 최대 길이 조정\n",
    "    batch_size=8  # 배치 크기 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"모델 타입:\", type(merged_model))\n",
    "print(\"토크나이저 타입:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "역할: 당신은 건설 안전 전문가입니다. 검색 결과를 바탕으로 질문에 대한 조치 사항을 간결하게 작성하세요.\n",
    "\n",
    "질문에 대한 재발 방지 대책 및 향후 조치 계획만 간결하게 답변하세요.\n",
    "검색된 내용에 기반하여 조치를 작성하세요.\n",
    "목차, 번호, 특수기호 없이 핵심 내용만 서술하세요.\n",
    "답변에 불필요한 부연 설명, 연결어, 주어 제거하세요.\n",
    "반드시 마침표로 끝나는 문장으로 작성하세요.\n",
    "반드시 답변만 출력하세요.\n",
    "\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 커스텀 프롬프트 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8XYUR_LNHll"
   },
   "source": [
    "# Result Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Encoder 모델 초기화 (한국어 모델 사용, max_length=512)\n",
    "cross_encoder = CrossEncoder(\"bongsoo/albert-small-kor-cross-encoder-v1\", max_length=512)\n",
    "# TensorFloat32 경고 해결 (필요시)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "tokenizer.padding_side = \"left\"  # tokenizer 객체가 있을 경우에만 사용\n",
    "\n",
    "# 배치 크기를 줄여서 GPU 메모리 부담을 줄입니다.\n",
    "batch_size = 8  # 예: 기존 16에서 4로 조정\n",
    "\n",
    "test_questions = combined_test_data['question'].tolist()\n",
    "test_dataset = Dataset.from_dict({\"question\": test_questions})\n",
    "val_results = []\n",
    "print(\"테스트 실행 시작... 총 테스트 샘플 수:\", len(test_questions))\n",
    "\n",
    "def process_batch(batch):\n",
    "    batched_prompts = []\n",
    "    retrieval_results = []\n",
    "    for q in batch[\"question\"]:\n",
    "        # 1. Dense Retrieval으로 관련 문서(청크) 전체 가져오기\n",
    "        dense_results = retriever.invoke(q)\n",
    "        if dense_results:\n",
    "            # 2. 각 (질문, 문서) 쌍에 대해 Cross-Encoder 점수 계산\n",
    "            pairs = [(q, doc.page_content if hasattr(doc, \"page_content\") else doc) for doc in dense_results]\n",
    "            ce_scores = cross_encoder.predict(pairs)\n",
    "            # 3. 점수를 기준으로 내림차순 정렬 후 55% 이상인 문서 필터링\n",
    "            reranked = sorted(zip(dense_results, ce_scores), key=lambda x: x[1], reverse=True)\n",
    "            filtered_docs = [doc for doc, score in reranked if score >= 0.55]\n",
    "            # 4. 조건에 만족하면 해당 문서들의 page_content를 결합하여 컨텍스트 생성, 아니면 빈 문자열\n",
    "            if filtered_docs:\n",
    "                context = \"\\n\".join([doc.page_content for doc in filtered_docs])\n",
    "            else:\n",
    "                context = \"\"\n",
    "\n",
    "            # 검색 결과 저장\n",
    "            retrieval_results.append({\n",
    "                \"question\": q,\n",
    "                \"retrieved_docs\": [\n",
    "                    {\n",
    "                        \"content\": doc.page_content if hasattr(doc, \"page_content\") else doc,\n",
    "                        \"similarity_score\": score\n",
    "                    } for doc, score in reranked if score >= 0.55\n",
    "                ]\n",
    "            })\n",
    "        else:\n",
    "            context = \"\"\n",
    "            retrieval_results.append({\n",
    "                \"question\": q,\n",
    "                \"retrieved_docs\": []\n",
    "            })\n",
    "        batched_prompts.append(prompt.format(context=context, question=q))\n",
    "\n",
    "    # 모델 추론 시 기울기 계산 방지를 위해 no_grad 블록 사용\n",
    "    with torch.no_grad():\n",
    "        outputs = text_generation_pipeline(batched_prompts, batch_size=len(batched_prompts))\n",
    "    # 각 출력 결과에서 'generated_text' 키의 값을 추출합니다.\n",
    "    return [output[0][\"generated_text\"] for output in outputs], retrieval_results\n",
    "\n",
    "# 메인 루프 수정 (배치 처리 후 캐시 클리어)\n",
    "for batch in tqdm(test_dataset.batch(batch_size), desc=\"검증 배치 처리\"):\n",
    "    batch_results, batch_retrieval_results = process_batch(batch)\n",
    "    val_results.extend(batch_results)\n",
    "    # 배치 처리 후 GPU 캐시를 비워 메모리 누수를 방지합니다.\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"\\n테스트 처리 완료! 총 검증 결과 수:\", len(val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame({\n",
    "     \"answer\": val_results  # 모델이 생성한 답변 리스트\n",
    "})\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CE4g-nY1k0s"
   },
   "source": [
    "# post_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_cleaning(df):\n",
    "  # 1번. 1., 2. 등 제거\n",
    "  df['answer'] = df['answer'].str.replace(r'^\\d+\\.', ',', regex=True)\n",
    "\n",
    "  # 2번. 줄 띄움 -> , 으로 변경\n",
    "  df['answer'] = df['answer'].str.replace('\\n','', regex=False)\n",
    "\n",
    "  # 3번. 질문부터 끝까지 삭제(있다면)\n",
    "  df['answer'] = df['answer'].str.replace(r'질문.*', '', regex=True)\n",
    "\n",
    "  # 4번. 연속된 \",\"를 \",\" 하나로 변경\n",
    "  df['answer'] = df['answer'].str.replace(r',+', '.', regex=True)\n",
    "\n",
    "  # 5번. 앞뒤 공백 및 \",\" 제거\n",
    "  df['answer'] = df['answer'].str.strip()\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = post_cleaning(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyMaxG2rqlug"
   },
   "source": [
    "# Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"\n",
    "embedding = SentenceTransformer(embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 리스트를 입력하여 임베딩 생성\n",
    "pred_embeddings = embedding.encode(test_result['answer'])\n",
    "print(pred_embeddings.shape)  # (샘플 개수, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(base_path / 'sample_submission.csv', encoding = 'utf-8-sig') # 수정해야함\n",
    "# 최종 결과 저장\n",
    "submission.iloc[:,1] = test_result['answer']\n",
    "submission.iloc[:,2:] = pred_embeddings\n",
    "submission.head()\n",
    "# 최종 결과를 CSV로 저장\n",
    "submission.to_csv(base_path / 'submission.csv', index=False, encoding='utf-8-sig') # 수정해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
