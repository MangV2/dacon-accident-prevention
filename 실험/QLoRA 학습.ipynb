{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U datasets\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trl\n",
    "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # 4비트 로드 활성화\n",
    "    bnb_4bit_quant_type=\"nf4\",           # 양자화 방식 (예: \"nf4\" 또는 \"fp4\")\n",
    "    bnb_4bit_use_double_quant=True,      # 이중 양자화 사용 여부\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # 연산 시 사용할 데이터 타입\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,              # 리포지토리의 custom 코드를 실행하도록 설정\n",
    "    quantization_config=quant_config,      # BitsAndBytesConfig 객체 전달\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Exaone 모델 구조에 맞게 target_modules 목록을 수정\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                     # LoRA 가중치 행렬의 rank (값이 작을수록 trainable parameter 수가 줄어듦)\n",
    "    lora_alpha=8,            # LoRA 스케일링 팩터\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'out_proj', 'c_fc_0', 'c_fc_1', 'c_proj'],\n",
    "    bias='none',             # bias 파라미터 학습 여부: 'none', 'all', 'lora_only'\n",
    "    task_type=\"CAUSAL_LM\"     # 문자열로 task_type 지정 (이전에는 TaskType.CAUSAL_LM을 사용했음)\n",
    ")\n",
    "\n",
    "# 양자화된 모델 학습 전 전처리\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# PEFT를 사용해 모델에 LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 학습 가능한 파라미터 출력\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/content/drive/MyDrive/train.csv\", encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "train['공사종류(대분류)'] = train['공사종류'].str.split(' / ').str[0]\n",
    "train['공사종류(중분류)'] = train['공사종류'].str.split(' / ').str[1]\n",
    "train['공종(대분류)'] = train['공종'].str.split(' > ').str[0]\n",
    "train['공종(중분류)'] = train['공종'].str.split(' > ').str[1]\n",
    "train['사고객체(대분류)'] = train['사고객체'].str.split(' > ').str[0]\n",
    "train['사고객체(중분류)'] = train['사고객체'].str.split(' > ').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 통합 생성\n",
    "combined_training_data = train.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"작업 프로세스는 '{row['작업프로세스']}'이며 {row['인적사고']}발생, 사고 원인은 '{row['사고원인']}'입니다.\"\n",
    "            f\"재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "        ),\n",
    "        \"answer\": row[\"재발방지대책 및 향후조치계획\"]\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "combined_training_data = pd.DataFrame(list(combined_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# combined_training_data가 Pandas DataFrame인 경우\n",
    "dataset_hf = Dataset.from_pandas(combined_training_data.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### 지침: 당신은 건설 안전 전문가입니다.\n",
    "질문에 대한 재발 방지 대책 및 향후 조치 계획만 간결하게 답변하세요.\n",
    "- 서론, 배경 설명, 추가 설명 없이 핵심 내용만 전달하세요.\n",
    "- 불필요한 형식(목차, 강조 표시, 리스트 등)을 사용하지 마세요.\n",
    "- 한 문장 또는 간결한 문단으로 자연스럽게 작성하세요.\n",
    "- 특수문자를 포함하지 마세요.\n",
    "\n",
    "{context}\n",
    "\n",
    "### 질문:\n",
    "{question}\n",
    "\n",
    "### 답변:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # 추가 context가 필요한 경우 여기서 설정 (현재는 빈 문자열)\n",
    "    context = \"\"\n",
    "    question = data_point[\"question\"]\n",
    "    # prompt_template을 사용해 prompt를 생성합니다.\n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "    # 모델 입력으로 사용할 프롬프트와 정답(평가용)만 반환합니다.\n",
    "    return {\"text\": prompt, \"answer\": data_point[\"answer\"]}\n",
    "\n",
    "# 기존 데이터셋의 모든 컬럼을 제거하고, generate_prompt에서 필요한 컬럼만 남깁니다.\n",
    "remove_column_keys = list(dataset_hf.features.keys())\n",
    "dataset_cvted = dataset_hf.shuffle(seed=42).map(generate_prompt, remove_columns=remove_column_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cvted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_cvted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징 완료된 데이터셋을 train/test로 분리\n",
    "split_datasets = dataset_cvted.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]  # 보통 검증 용도로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_column_keys = train_dataset.features.keys()\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_function, batched=True, remove_columns=remove_column_keys)\n",
    "\n",
    "remove_column_keys = eval_dataset.features.keys()\n",
    "test_dataset_tokenized = eval_dataset.map(tokenize_function, batched=True, remove_columns=remove_column_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    examples_batch = tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "    examples_batch['labels'] = examples_batch['input_ids'] # 모델 학습 평가를 위한 loss 계산을 위해 입력 토큰을 레이블로 사용\n",
    "    return examples_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TrainingArguments에 evaluation_strategy와 eval_steps 추가\n",
    "train_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=8,         # 각 디바이스당 배치 사이즈\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=1,\n",
    "    max_steps=2000,\n",
    "    learning_rate=2e-4,                     # 학습률\n",
    "    bf16=True,                              # bf16 사용 (하드웨어 지원 확인 필요)\n",
    "    output_dir=\"/content/drive/MyDrive/model_weight\",\n",
    "    optim=\"paged_adamw_8bit\",               # 8비트 AdamW 옵티마이저\n",
    "    logging_steps=50,                       # 로깅 빈도\n",
    "    save_total_limit=3,                     # 저장할 체크포인트의 최대 수\n",
    "    evaluation_strategy=\"steps\",            # 일정 스텝마다 평가 실행\n",
    "    eval_steps=50                           # 평가 주기 (50 스텝마다 평가)\n",
    ")\n",
    "\n",
    "# 3. SFTTrainer에 eval_dataset을 추가하여 수정\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=train_args,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 - > 이때 qLoRA를 이용해서 학습한 파라미터만 저장되서 작음\n",
    "FINETUNED_MODEL = \"qlora\"\n",
    "trainer.model.save_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불러오기\n",
    "FINETUNED_MODEL = \"/content/qlora\"\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 모델 및 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA 모델 로드\n",
    "peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA 가중치를 베이스 모델에 병합\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 TEST\n",
    "prompt = dataset_cvted[2]['text']\n",
    "# 텍스트 생성을 위한 파이프라인 설정\n",
    "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer, max_new_tokens=64)\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    #top_k=0,\n",
    "    #top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cvted[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터 전처리\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. CSV 파일 로드 (예: test.csv)\n",
    "test_df = pd.read_csv(\"/content/drive/MyDrive/test.csv\")\n",
    "\n",
    "# 데이터 전처리\n",
    "test_df['공사종류(대분류)'] = test_df['공사종류'].str.split(' / ').str[0]\n",
    "test_df['공사종류(중분류)'] = test_df['공사종류'].str.split(' / ').str[1]\n",
    "test_df['공종(대분류)'] = test_df['공종'].str.split(' > ').str[0]\n",
    "test_df['공종(중분류)'] = test_df['공종'].str.split(' > ').str[1]\n",
    "test_df['사고객체(대분류)'] = test_df['사고객체'].str.split(' > ').str[0]\n",
    "test_df['사고객체(중분류)'] = test_df['사고객체'].str.split(' > ').str[1]\n",
    "\n",
    "# 2. train 데이터와 동일한 방식으로 prompt 생성\n",
    "def generate_prompt_from_row(row):\n",
    "    prompt = (\n",
    "         f\"작업 프로세스는 '{row['작업프로세스']}'이며 {row['인적사고']}발생, 사고 원인은 '{row['사고원인']}'입니다.\"\n",
    "            f\"재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "    )\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "# DataFrame의 각 행에 대해 prompt 생성 (새로운 열로 추가)\n",
    "test_df = test_df.apply(generate_prompt_from_row, axis=1)\n",
    "test_df = pd.DataFrame(test_df.tolist())\n",
    "\n",
    "# 3. HuggingFace Dataset으로 변환\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    # 추가 context가 필요한 경우 여기서 설정 (현재는 빈 문자열)\n",
    "    context = \"\"\n",
    "    question = data_point[\"prompt\"]\n",
    "    # prompt_template을 사용해 prompt를 생성합니다.\n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "    # 모델 입력으로 사용할 프롬프트와 정답(평가용)만 반환합니다.\n",
    "    return {\"prompt\": prompt,}\n",
    "\n",
    "remove_column_keys = list(test_dataset.features.keys())\n",
    "test_dataset = test_dataset.map(generate_prompt, remove_columns=remove_column_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 가정: test_dataset은 HuggingFace Dataset 객체이며, merged_model과 tokenizer가 이미 정의되어 있음.\n",
    "# 예시: test_dataset.features = ['prompt']\n",
    "\n",
    "# 텍스트 생성을 위한 파이프라인 설정\n",
    "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer, max_new_tokens=64)\n",
    "\n",
    "# 배치 사이즈 설정 (메모리 상황에 맞게 조정)\n",
    "batch_size = 8\n",
    "test_results = []\n",
    "\n",
    "# HuggingFace Dataset은 slicing 시 dict 형태로 각 컬럼을 반환할 수 있음\n",
    "for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "    # i부터 i+batch_size 까지 슬라이싱하여 배치 데이터 추출\n",
    "    batch = test_dataset[i:i+batch_size]\n",
    "    # \"prompt\" 컬럼의 리스트를 추출 (예: batch[\"prompt\"]가 리스트 형태)\n",
    "    batch_prompts = batch[\"prompt\"]\n",
    "\n",
    "    # 배치로 텍스트 생성 (한 번에 여러 프롬프트 처리)\n",
    "    batch_outputs = pipe(\n",
    "        batch_prompts,\n",
    "        do_sample=False,\n",
    "        temperature=0.1,\n",
    "        repetition_penalty=1.2,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # 각 프롬프트에 대해 첫 번째 생성 결과를 이용하여, 원본 프롬프트 이후의 텍스트만 추출\n",
    "    for prompt, outputs in zip(batch_prompts, batch_outputs):\n",
    "        generated_text = outputs[0][\"generated_text\"][len(prompt):]\n",
    "        test_results.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 텍스트 리스트에 대해 임베딩 생성\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"\n",
    "embedding = SentenceTransformer(embedding_model_name)\n",
    "pred_embeddings = embedding.encode(test_results)\n",
    "print(pred_embeddings.shape)  # (샘플 개수, 768)\n",
    "\n",
    "# sample_submission 파일 불러오기\n",
    "submission = pd.read_csv('/content/drive/MyDrive/sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 최종 결과 저장: 두 번째 열에 생성 텍스트, 이후 열에 임베딩 벡터 할당\n",
    "submission.iloc[:, 1] = test_results\n",
    "submission.iloc[:, 2:] = pred_embeddings\n",
    "\n",
    "print(submission.head())\n",
    "\n",
    "# 최종 결과 CSV로 저장\n",
    "submission.to_csv('exaone+QLoRA_V1.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
