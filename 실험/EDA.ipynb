{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/minkyu/Desktop/open/\"\n",
    "base_path = 'C:/Users/minkyu/Desktop/dacon accident prevention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path+\"train.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['인적사고'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"1. **작업 표면 안전 확보**: 모든 작업 플랫폼에 미끄럼 방지 매트 설치 및 정기적인 점검 실시.\n",
    "2. **안전 교육 강화**: 미끄러짐 위험 인식과 안전한 작업 방법에 대한 정기 교육 실시.\n",
    "3. **개인 보호 장비 의무화**: 안전화 및 안전벨트 착용을 강제화하며, 특히 미끄러운 환경에서는 추가적인 미끄럼 방지 기능이 있는 장비 사용 권장.\n",
    "4. **작업 절차 표준화**: 자재 이동 및 해체 작업 시 안전한 절차를 명확히 정의하고 준수를 감독.\n",
    "5. **정기적 안전 점검**: 해체 작업\"\"\"\n",
    "\n",
    "# 숫자 + 점(.) 형태 제거 (예: \"1.\", \"2.\" 등)\n",
    "text = re.sub(r'\\d+\\.', '', text)\n",
    "\n",
    "# **내용** 패턴 제거\n",
    "text = re.sub(r'\\*\\*.*?\\*\\*', '', text)\n",
    "\n",
    "# : 제거\n",
    "text = re.sub(r':', '', text)\n",
    "\n",
    "# 마지막 .을 제외한 모든 .을 ,로 변경\n",
    "text = re.sub(r'\\.(?=.*[^.])', ',', text)\n",
    "\n",
    "# 줄바꿈 제거\n",
    "text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "# 2칸 이상 띄어쓰기 제거 (한 칸으로 변환)\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "print(text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/minkyu/Desktop/dacon accident prevention/submission-EXAONE-3.5-32B-Instruct.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['재발방지대책 및 향후조치계획']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식을 적용하는 함수 정의\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\d+\\.', '', text)          # 숫자 + 점(.) 제거 (예: \"1.\", \"2.\")\n",
    "    text = re.sub(r'\\*\\*.*?\\*\\*', '', text)    # **내용** 제거\n",
    "    text = re.sub(r':', '', text)              # : 제거\n",
    "    text = re.sub(r'\\.(?=.*[^.])', ',', text)  # 마지막 .을 제외한 모든 .을 ,로 변경\n",
    "    text = re.sub(r'\\n', ' ', text)            # 줄바꿈 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)           # 2칸 이상 띄어쓰기 제거 (한 칸으로 변환)\n",
    "    return text.strip()\n",
    "\n",
    "# 데이터프레임에 적용\n",
    "df['재발방지대책 및 향후조치계획'] = df['재발방지대책 및 향후조치계획'].apply(clean_text)\n",
    "\n",
    "# 결과 확인\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['재발방지대책 및 향후조치계획']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'C:/Users/minkyu/Desktop/dacon accident prevention'\n",
    "sample = pd.read_csv(data_path+\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"\n",
    "embedding = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "# 5. 정제된 문장을 리스트로 변환 후 임베딩\n",
    "test_results = df['재발방지대책 및 향후조치계획'].tolist()\n",
    "pred_embeddings = embedding.encode(test_results)  # (샘플 개수, 768)\n",
    "\n",
    "# 6. submission 파일 로드\n",
    "submission = pd.read_csv(data_path+\"sample_submission.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# 7. submission 파일에 결과 저장\n",
    "submission.iloc[:, 1] = test_results  # 2번째 컬럼에 정제된 문장 저장\n",
    "submission.iloc[:, 2:] = pred_embeddings  # 3번째 컬럼부터 임베딩 저장\n",
    "\n",
    "# 8. CSV 저장\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission-EXAONE-3.5-32B-Instruct_정규식적용.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(jhgan/ko-sbert-sts) 불러오기\n",
    "model = SentenceTransformer('jhgan/ko-sbert-sts', use_auth_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/minkyu/Desktop/open/\"\n",
    "base_path = 'C:/Users/minkyu/Desktop/dacon accident prevention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path+\"train.csv\")\n",
    "test = pd.read_csv(data_path+\"test.csv\")\n",
    "sample = pd.read_csv(data_path+\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = train.groupby(\"인적사고\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "cosine_res = []\n",
    "for name, group in tqdm(grouped):\n",
    "    plan = group[\"재발방지대책 및 향후조치계획\"]\n",
    "    vectors = np.stack(plan.apply(model.encode).to_numpy())\n",
    "    similarity = cosine_similarity(vectors, vectors)    \n",
    "    cosine_res += similarity[similarity.mean(axis=1).argmax()].tolist()\n",
    "    res[name] = plan.iloc[similarity.mean(axis=1).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = cosine_res\n",
    "\n",
    "# 0.1 단위로 구간을 지정\n",
    "bins = np.arange(0, 1.1, 0.1)  # 0.0 ~ 1.0을 0.1 간격으로 나눔\n",
    "\n",
    "# 히스토그램 계산\n",
    "hist, bin_edges = np.histogram(arr, bins=bins)\n",
    "\n",
    "# 결과 출력\n",
    "for i in range(len(hist)):\n",
    "    print(f\"Range {bin_edges[i]:.1f} - {bin_edges[i+1]:.1f}: {hist[i]}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 모델 로드\n",
    "model = SentenceTransformer('jhgan/ko-sbert-sts', use_auth_token=False)\n",
    "\n",
    "# 결과를 저장할 딕셔너리\n",
    "category_cosine_res = {}  # 카테고리별 유사도 저장\n",
    "res = {}  # 대표 문장 저장\n",
    "\n",
    "# 그룹화 및 처리\n",
    "grouped = train.groupby(\"인적사고\")\n",
    "for name, group in tqdm(grouped):\n",
    "    plan = group[\"재발방지대책 및 향후조치계획\"]\n",
    "    vectors = np.stack(plan.apply(model.encode).to_numpy())\n",
    "    similarity = cosine_similarity(vectors, vectors)\n",
    "    \n",
    "    # 평균 유사도가 가장 높은 행의 유사도 값 저장\n",
    "    max_mean_idx = similarity.mean(axis=1).argmax()\n",
    "    category_cosine_res[name] = similarity[max_mean_idx].tolist()  # 해당 카테고리의 유사도 리스트\n",
    "    res[name] = plan.iloc[max_mean_idx]  # 대표 문장 저장\n",
    "\n",
    "# 범위별 카테고리 분포 계산\n",
    "bins = np.arange(0, 1.1, 0.1)\n",
    "category_bin_counts = {name: np.histogram(values, bins=bins)[0] for name, values in category_cosine_res.items()}\n",
    "\n",
    "# 결과 출력\n",
    "print(\"범위별 '인적사고' 카테고리 분포:\")\n",
    "for i in range(len(bins) - 1):\n",
    "    print(f\"\\nRange {bins[i]:.1f} - {bins[i+1]:.1f}:\")\n",
    "    for category, counts in category_bin_counts.items():\n",
    "        if counts[i] > 0:  # 해당 범위에 값이 있는 경우만 출력\n",
    "            print(f\"  {category}: {counts[i]}개\")\n",
    "\n",
    "# 전체 히스토그램 재확인\n",
    "all_cosine_res = []\n",
    "for values in category_cosine_res.values():\n",
    "    all_cosine_res.extend(values)\n",
    "hist, bin_edges = np.histogram(all_cosine_res, bins=bins)\n",
    "print(\"\\n전체 히스토그램 확인:\")\n",
    "for i in range(len(hist)):\n",
    "    print(f\"Range {bin_edges[i]:.1f} - {bin_edges[i+1]:.1f}: {hist[i]}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_bin_counts = {name: np.histogram(values, bins=bins)[0] for name, values in category_cosine_res.items()}\n",
    "\n",
    "# 결과 출력: 개수와 비율\n",
    "print(\"범위별 '인적사고' 카테고리 분포 (개수 및 비율):\")\n",
    "for i in range(len(bins) - 1):\n",
    "    print(f\"\\nRange {bins[i]:.1f} - {bins[i+1]:.1f}:\")\n",
    "    for category, counts in category_bin_counts.items():\n",
    "        total_in_category = sum(counts)  # 해당 카테고리의 전체 유사도 값 개수\n",
    "        if counts[i] > 0:  # 해당 범위에 값이 있는 경우만 출력\n",
    "            percentage = (counts[i] / total_in_category) * 100\n",
    "            print(f\"  {category}: {counts[i]}개 ({percentage:.2f}%)\")\n",
    "\n",
    "# 전체 히스토그램 재확인\n",
    "all_cosine_res = []\n",
    "for values in category_cosine_res.values():\n",
    "    all_cosine_res.extend(values)\n",
    "hist, bin_edges = np.histogram(all_cosine_res, bins=bins)\n",
    "print(\"\\n전체 히스토그램 확인:\")\n",
    "for i in range(len(hist)):\n",
    "    print(f\"Range {bin_edges[i]:.1f} - {bin_edges[i+1]:.1f}: {hist[i]}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 모델 로드\n",
    "model = SentenceTransformer('jhgan/ko-sbert-sts', use_auth_token=False)\n",
    "\n",
    "# 결과를 저장할 딕셔너리\n",
    "category_cosine_res = {}  # 카테고리별 유사도 값 저장\n",
    "category_sentences = {}   # 카테고리별 문장과 유사도 저장\n",
    "res = {}                  # 대표 문장 저장\n",
    "\n",
    "# 그룹화 및 처리\n",
    "grouped = train.groupby(\"인적사고\")\n",
    "for name, group in tqdm(grouped):\n",
    "    plan = group[\"재발방지대책 및 향후조치계획\"]\n",
    "    vectors = np.stack(plan.apply(model.encode).to_numpy())\n",
    "    similarity = cosine_similarity(vectors, vectors)\n",
    "    \n",
    "    # 평균 유사도가 가장 높은 행 선택\n",
    "    max_mean_idx = similarity.mean(axis=1).argmax()\n",
    "    cosine_values = similarity[max_mean_idx].tolist()  # 해당 행의 유사도 값들\n",
    "    sentences = plan.tolist()                          # 모든 문장 리스트\n",
    "    \n",
    "    # 카테고리별 유사도와 문장 저장\n",
    "    category_cosine_res[name] = cosine_values\n",
    "    category_sentences[name] = list(zip(cosine_values, sentences))  # (유사도, 문장) 쌍으로 저장\n",
    "    res[name] = plan.iloc[max_mean_idx]  # 대표 문장 저장\n",
    "\n",
    "# 범위 설정\n",
    "bins = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "# 특정 범위(0.6~1.0)만 추출\n",
    "target_ranges = [(0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
    "\n",
    "# 범위별 문장 출력\n",
    "print(\"범위별 '인적사고' 카테고리 문장 (최대 10개씩):\")\n",
    "for lower, upper in target_ranges:\n",
    "    print(f\"\\nRange {lower:.1f} - {upper:.1f}:\")\n",
    "    for category, sentence_pairs in category_sentences.items():\n",
    "        # 해당 범위에 속하는 (유사도, 문장) 쌍 필터링\n",
    "        filtered_sentences = [(sim, sent) for sim, sent in sentence_pairs if lower <= sim < upper]\n",
    "        if filtered_sentences:\n",
    "            print(f\"  {category}:\")\n",
    "            # 최대 10개 출력\n",
    "            for sim, sent in filtered_sentences[:10]:\n",
    "                print(f\"    - {sent} (유사도: {sim:.3f})\")\n",
    "            if len(filtered_sentences) > 10:\n",
    "                print(f\"      ... (총 {len(filtered_sentences)}개)\")\n",
    "\n",
    "# 전체 히스토그램 재확인\n",
    "all_cosine_res = []\n",
    "for values in category_cosine_res.values():\n",
    "    all_cosine_res.extend(values)\n",
    "hist, bin_edges = np.histogram(all_cosine_res, bins=bins)\n",
    "print(\"\\n전체 히스토그램 확인:\")\n",
    "for i in range(len(hist)):\n",
    "    print(f\"Range {bin_edges[i]:.1f} - {bin_edges[i+1]:.1f}: {hist[i]}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 파일로 저장\n",
    "with open('output.txt', 'w', encoding='utf-8') as f:\n",
    "    # 범위별 문장 출력\n",
    "    f.write(\"범위별 '인적사고' 카테고리 문장 (최대 10개씩):\\n\")\n",
    "    for lower, upper in target_ranges:\n",
    "        f.write(f\"\\nRange {lower:.1f} - {upper:.1f}:\\n\")\n",
    "        for category, sentence_pairs in category_sentences.items():\n",
    "            # 해당 범위에 속하는 (유사도, 문장) 쌍 필터링\n",
    "            filtered_sentences = [(sim, sent) for sim, sent in sentence_pairs if lower <= sim < upper]\n",
    "            if filtered_sentences:\n",
    "                f.write(f\"  {category}:\\n\")\n",
    "                # 최대 10개 출력\n",
    "                for sim, sent in filtered_sentences[:10]:\n",
    "                    f.write(f\"    - {sent} (유사도: {sim:.3f})\\n\")\n",
    "                if len(filtered_sentences) > 10:\n",
    "                    f.write(f\"      ... (총 {len(filtered_sentences)}개)\\n\")\n",
    "\n",
    "    # 전체 히스토그램 재확인\n",
    "    all_cosine_res = []\n",
    "    for values in category_cosine_res.values():\n",
    "        all_cosine_res.extend(values)\n",
    "    hist, bin_edges = np.histogram(all_cosine_res, bins=bins)\n",
    "    f.write(\"\\n전체 히스토그램 확인:\\n\")\n",
    "    for i in range(len(hist)):\n",
    "        f.write(f\"Range {bin_edges[i]:.1f} - {bin_edges[i+1]:.1f}: {hist[i]}개\\n\")\n",
    "\n",
    "print(\"결과가 'output.txt' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(jhgan/ko-sbert-sts) 불러오기\n",
    "model = SentenceTransformer('jhgan/ko-sbert-sts', use_auth_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 예시\n",
    "preds = [\n",
    "    \"작업 전 작업자 안전교육 및 안전관리 철저에 대한 재발 방지 대책과 향후 조치 계획\",\n",
    "]\n",
    "\n",
    "gts = [\n",
    "    \"위험성 평가 및 교육을 통해 작업장 내 위험요인과 안전수칙을 근로자에게 전파하고, 근로자 안전교육을 강화하며, 본 사고와 관련된 유사 피해 발생 방지를 위한 안전교육을 실시할 계획.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플에 대한 Cosine Similarity 산식\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b) if norm_a != 0 and norm_b != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_scores = []\n",
    "for pred, gt in zip(preds, gts):\n",
    "    # 생성된 답변 내용을 768 Embedding Vector로 변환\n",
    "    pred_embed = model.encode(pred)\n",
    "    gt_embed = model.encode(gt)\n",
    "    \n",
    "    sample_score = cosine_similarity(gt_embed, pred_embed)\n",
    "    # Cosine Similarity Score가 0보다 작으면 0으로 간주\n",
    "    sample_score = max(sample_score, 0)\n",
    "    print('예측 : ', pred)\n",
    "    print('정답 : ', gt)\n",
    "    print('Cosine Similarity Score : ', sample_score)\n",
    "    print('-'*20)\n",
    "    sample_scores.append(sample_score)\n",
    "print('전체 샘플의 Cosine Similarity Score 평균 : ', np.mean(sample_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
