{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aJdMCJdQ0WQ"
   },
   "source": [
    "# Vector Store Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface\n",
    "!pip install faiss-gpu-cu12\n",
    "!pip langchain_text_splitters\n",
    "!pip install langchain_community\n",
    "!pip install pypdfium2\n",
    "!pip install kiwipiepy\n",
    "!pip install rank_bm25\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trlmm\n",
    "!pip install -U transformers\n",
    "!pip install faiss-gpu-cu12\n",
    "!pip install trl\n",
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import CrossEncoder\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.cwd()\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF íŒŒì¼ ì „ì²˜ë¦¬\n",
    "def preprocessing_pdf(text):\n",
    "    \"\"\"ìœ„ì—ì„œ 3ì¤„ ì‚­ì œ í›„, íŠ¹ì • íŒ¨í„´ ì œê±°\"\"\"\n",
    "\n",
    "    # ë§¨ ìœ„ 3ì¤„ ì‚­ì œ\n",
    "    lines = text.split(\"\\n\")  # ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°\n",
    "    text = \"\\n\".join(lines[3:])  # ì• 3ì¤„ ì‚­ì œ í›„ ë‹¤ì‹œ í•©ì¹˜ê¸°\n",
    "\n",
    "    # 'KOSHA Guide' ë˜ëŠ” 'KOSHA GUIDE' ë’¤ì˜ ëª¨ë“  ë¬¸ì ì‚­ì œ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ O)\n",
    "    text = re.sub(r'KOSHA GUIDE.*|KOSHA Guide.*', '', text)\n",
    "\n",
    "    # 'C - 'ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì‚­ì œ (MULTILINE)\n",
    "    text = re.sub(r'^C - .*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # '<ê·¸ë¦¼'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì‚­ì œ\n",
    "    text = re.sub(r'^<ê·¸ë¦¼.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # '- ìˆ«ì -' íŒ¨í„´ ì‚­ì œ\n",
    "    text = re.sub(r'^\\s*- \\d+ -\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # ìœ ë‹ˆì½”ë“œ ë¹„í‘œì¤€ ë¬¸ì(ê¹¨ì§„ ë¬¸ì) ì œê±° (Private Use Area, PUA ë¬¸ì ì œê±°)\n",
    "    text = re.sub(r'[\\ue000-\\uf8ff]', '', text)  # U+E000 ~ U+F8FF ë²”ìœ„ ì œê±°\n",
    "\n",
    "    return text.strip()  # ì•ë’¤ ê³µë°± ì œê±°\n",
    "\n",
    "# í´ë” ë‚´ ëª¨ë“  PDF íŒŒì¼ ì°¾ê¸°\n",
    "pdf_folder = base_path + \"/á„€á…¥á†«á„‰á…¥á†¯á„‹á…¡á†«á„Œá…¥á†«á„Œá…µá„á…µá†·/\" # ìˆ˜ì • í•´ì•¼í•¨\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "\n",
    "print(f\" ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "\n",
    "\n",
    "#  ë¬¸ì„œ ë¶„í• ê¸° ì„¤ì • (500ì ë‹¨ìœ„, 50ì ì¤‘ì²©)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "#  ì „ì²´ ë¬¸ì„œë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "all_splits = []\n",
    "\n",
    "#  PDF íŒŒì¼ë³„ ì²˜ë¦¬\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"ğŸ” ì²˜ë¦¬ ì¤‘: {pdf_path}\")\n",
    "\n",
    "    #  PDF ë¡œë“œ\n",
    "    loader = PyPDFium2Loader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    #  ì „ì²˜ë¦¬ ì ìš© (ëª¨ë“  í˜ì´ì§€)\n",
    "    processed_documents = [preprocessing_pdf(doc.page_content) for doc in documents]\n",
    "\n",
    "    #  3í˜ì´ì§€(ì¸ë±ìŠ¤ 2)ë¶€í„° ë¬¸ì„œ ë¶„í•  ë° ì €ì¥\n",
    "    for doc in processed_documents[3:]:  # âœ… 3í˜ì´ì§€ë¶€í„° ì²˜ë¦¬\n",
    "        splits = text_splitter.split_text(doc)  # ê°œë³„ í˜ì´ì§€ ë¶„í• \n",
    "        all_splits.extend(splits)  # ëª¨ë“  ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "        all_splits = [chunk.replace(\"\\n\\n\", \"[PARA]\").replace(\"\\n\", \" \").replace(\"[PARA]\", \"\\n\\n\") for chunk in all_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâœ… ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“ ìƒì„±ëœ ì´ ì²­í¬ ê°œìˆ˜: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"  # í•œêµ­ì–´ SBERT ëª¨ë¸\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# ì§„í–‰ ë°” ì¶”ê°€í•˜ì—¬ ì„ë² ë”© ìƒì„± (íš¨ìœ¨ì ì¸ ë°©ì‹)\n",
    "embeddings_list = [embedding.embed_query(text) for text in tqdm(all_splits, desc=\"ì„ë² ë”© ì§„í–‰ ì¤‘\", unit=\"chunk\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings_list), len(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (í…ìŠ¤íŠ¸, ì„ë² ë”©) ìŒì„ ìƒì„±\n",
    "text_embedding_pairs = list(zip(all_splits, embeddings_list))\n",
    "\n",
    "# FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embedding_pairs,  # ì˜¬ë°”ë¥¸ í˜•ì‹ (íŠœí”Œ ë¦¬ìŠ¤íŠ¸)\n",
    "    embedding=embedding  # ì„ë² ë”© ëª¨ë¸ ê°ì²´\n",
    ")\n",
    "\n",
    "print(\"FAISS ë²¡í„° ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAISSì— ì €ì¥ëœ ë²¡í„° ê°œìˆ˜:\", vector_store.index.ntotal)\n",
    "print(\"ì‹¤ì œ embeddings ê°œìˆ˜:\", len(embeddings_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLplxo55Lsdo"
   },
   "source": [
    "# LoRA model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "FINETUNED_MODEL = base_path + \"/ko_gemma\" # ìˆ˜ì • í•´ì•¼í•¨\n",
    "peft_config = PeftConfig.from_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–‘ìí™” ì„¤ì •\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # 4ë¹„íŠ¸ ë¡œë“œ í™œì„±í™”\n",
    "    bnb_4bit_quant_type=\"nf4\",           # ì–‘ìí™” ë°©ì‹ (ì˜ˆ: \"nf4\" ë˜ëŠ” \"fp4\")\n",
    "    bnb_4bit_use_double_quant=True,      # ì´ì¤‘ ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # ì—°ì‚° ì‹œ ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dynamo ì™„ì „ ë¹„í™œì„±í™” (ê°•ì œ)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._inductor.config.fallback_random = True\n",
    "torch._dynamo.reset()  # ê¸°ì¡´ ìºì‹œ ì œê±°\n",
    "torch._dynamo.config.cache_size_limit = 0  # ìºì‹œ í¬ê¸° ì œí•œ\n",
    "torch._dynamo.config.disable = True  # Dynamo ê°•ì œ ë¹„í™œì„±í™”\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ëª¨ë¸ ë¡œë“œ\n",
    "peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ëª¨ë¸ íƒ€ì…:\", type(peft_model))\n",
    "print(\"í† í¬ë‚˜ì´ì € íƒ€ì…:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ëª¨ë¸ íƒ€ì…:\", type(merged_model))\n",
    "print(\"í† í¬ë‚˜ì´ì € íƒ€ì…:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²½ë¡œ í™•ì¸\n",
    "test = pd.read_csv(base_path + '/test.csv', encoding = 'utf-8-sig') # ìˆ˜ì • í•´ì•¼í•¨\n",
    "\n",
    "test['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str[0]\n",
    "test['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str[1]\n",
    "test['ê³µì¢…(ëŒ€ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split(' > ').str[0]\n",
    "test['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split(' > ').str[1]\n",
    "test['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[0]\n",
    "test['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[1]\n",
    "test['ì¸ì ì‚¬ê³ '] = test['ì¸ì ì‚¬ê³ '].str.replace(r'\\(.*?\\)', '', regex=True)\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í†µí•© ìƒì„±\n",
    "combined_test_data = test.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"{row['ê³µì¢…(ì¤‘ë¶„ë¥˜)']} ì‘ì—… ì¤‘ {row['ì¸ì ì‚¬ê³ ']} ë°œìƒ. \\n\"\n",
    "            f\"í‚¤ì›Œë“œ: {row['ì‚¬ê³ ì›ì¸']} \\n\"\n",
    "            f\"{row['ì¸ì ì‚¬ê³ ']} ë°©ì§€ë¥¼ ìœ„í•œ ì¡°ì¹˜ëŠ”?\"\n",
    "        )\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "combined_test_data = pd.DataFrame(list(combined_test_data))\n",
    "combined_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (BM25 í† í¬ë‚˜ì´ì € ìš©ë„)\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def kiwi_tokenizer(text):\n",
    "    return [token.form for token in kiwi.tokenize(text)]  # ë¬¸ì¥ì„ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ”\n",
    "\n",
    "# ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ FAISS Vector Storeì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "docs = [Document(page_content=text) for text in all_splits]  # FAISSì˜ text ë°ì´í„°ë¥¼ Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "\n",
    "# FAISS ê¸°ë°˜ Retriever ìƒì„±\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# BM25 ê¸°ë°˜ Retriever ìƒì„± (í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ìš©)\n",
    "bm25_retriever = BM25Retriever.from_documents(docs, tokenizer=kiwi_tokenizer)\n",
    "bm25_retriever.k = 3\n",
    "\n",
    "# BM25 + FAISS ê²°í•© (ì•™ìƒë¸” ê²€ìƒ‰)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever],  # ë‘ ê°œì˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ ê²°í•©\n",
    "    weights=[0.8, 0.2]  # BM25ì™€ FAISSë¥¼ ë™ì¼í•œ ê°€ì¤‘ì¹˜( 8:2)ë¡œ ì„¤ì •\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model= merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False, # False ë¡œ í•˜ë©´ ê°™ì€ ì…ë ¥ì— ê°™ì€ ì¶œë ¥\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=128, # ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ ì¡°ì •\n",
    "    batch_size=8  # ë°°ì¹˜ í¬ê¸° ì§€ì •\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ëª¨ë¸ íƒ€ì…:\", type(merged_model))\n",
    "print(\"í† í¬ë‚˜ì´ì € íƒ€ì…:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "ì—­í• : ë‹¹ì‹ ì€ ê±´ì„¤ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì¡°ì¹˜ ì‚¬í•­ì„ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸ì— ëŒ€í•œ ì¬ë°œ ë°©ì§€ ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšë§Œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "ê²€ìƒ‰ëœ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ì¡°ì¹˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "ëª©ì°¨, ë²ˆí˜¸, íŠ¹ìˆ˜ê¸°í˜¸ ì—†ì´ í•µì‹¬ ë‚´ìš©ë§Œ ì„œìˆ í•˜ì„¸ìš”.\n",
    "ë‹µë³€ì— ë¶ˆí•„ìš”í•œ ë¶€ì—° ì„¤ëª…, ì—°ê²°ì–´, ì£¼ì–´ ì œê±°í•˜ì„¸ìš”.\n",
    "ë°˜ë“œì‹œ ë§ˆì¹¨í‘œë¡œ ëë‚˜ëŠ” ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "ë°˜ë“œì‹œ ë‹µë³€ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# RAG ì²´ì¸ ìƒì„±\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # stuff -> ê·¸ëƒ¥ í•©ì³ì„œ llmì— ì „ë‹¬ / map_reduce -> í•©ì¹œê±° ìš”ì•½í•´ì„œ llm ì „ë‹¬ / refine -> ìˆœì°¨ì ìœ¼ë¡œ ë¬¸ì„œ ì¶”ê°€í•´ì„œ ë‹µë³€ ë³´ì™„\n",
    "    retriever=ensemble_retriever,\n",
    "    return_source_documents=False, # ìƒì„±ëœ ë‹µë³€ê³¼ í•¨ê»˜ ì‚¬ìš©ëœ ì†ŒìŠ¤ ë¬¸ì„œë“¤ë„ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶œì²˜ ì•Œë ¤ì£¼ëŠ” ê°œë…\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(qa_chain))  # ì •ìƒì ìœ¼ë¡œ ì •ì˜ëœ ê°ì²´ì¸ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8XYUR_LNHll"
   },
   "source": [
    "# Result Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Encoder ëª¨ë¸ ì´ˆê¸°í™” (í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš©, max_length=512)\n",
    "cross_encoder = CrossEncoder(\"bongsoo/albert-small-kor-cross-encoder-v1\", max_length=512)\n",
    "# TensorFloat32 ê²½ê³  í•´ê²° (í•„ìš”ì‹œ)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "tokenizer.padding_side = \"left\"  # tokenizer ê°ì²´ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ì‚¬ìš©\n",
    "\n",
    "# ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ GPU ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì…ë‹ˆë‹¤.\n",
    "batch_size = 8  # ì˜ˆ: ê¸°ì¡´ 16ì—ì„œ 4ë¡œ ì¡°ì •\n",
    "\n",
    "test_questions = combined_test_data['question'].tolist()\n",
    "test_dataset = Dataset.from_dict({\"question\": test_questions})\n",
    "val_results = []\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘... ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜:\", len(test_questions))\n",
    "\n",
    "def process_batch(batch):\n",
    "    batched_prompts = []\n",
    "    retrieval_results = []\n",
    "    for q in batch[\"question\"]:\n",
    "        # 1. Dense Retrievalìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ(ì²­í¬) ì „ì²´ ê°€ì ¸ì˜¤ê¸°\n",
    "        dense_results = retriever.invoke(q)\n",
    "        if dense_results:\n",
    "            # 2. ê° (ì§ˆë¬¸, ë¬¸ì„œ) ìŒì— ëŒ€í•´ Cross-Encoder ì ìˆ˜ ê³„ì‚°\n",
    "            pairs = [(q, doc.page_content if hasattr(doc, \"page_content\") else doc) for doc in dense_results]\n",
    "            ce_scores = cross_encoder.predict(pairs)\n",
    "            # 3. ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ 55% ì´ìƒì¸ ë¬¸ì„œ í•„í„°ë§\n",
    "            reranked = sorted(zip(dense_results, ce_scores), key=lambda x: x[1], reverse=True)\n",
    "            filtered_docs = [doc for doc, score in reranked if score >= 0.55]\n",
    "            # 4. ì¡°ê±´ì— ë§Œì¡±í•˜ë©´ í•´ë‹¹ ë¬¸ì„œë“¤ì˜ page_contentë¥¼ ê²°í•©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±, ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´\n",
    "            if filtered_docs:\n",
    "                context = \"\\n\".join([doc.page_content for doc in filtered_docs])\n",
    "            else:\n",
    "                context = \"\"\n",
    "\n",
    "            # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥\n",
    "            retrieval_results.append({\n",
    "                \"question\": q,\n",
    "                \"retrieved_docs\": [\n",
    "                    {\n",
    "                        \"content\": doc.page_content if hasattr(doc, \"page_content\") else doc,\n",
    "                        \"similarity_score\": score\n",
    "                    } for doc, score in reranked if score >= 0.55\n",
    "                ]\n",
    "            })\n",
    "        else:\n",
    "            context = \"\"\n",
    "            retrieval_results.append({\n",
    "                \"question\": q,\n",
    "                \"retrieved_docs\": []\n",
    "            })\n",
    "        batched_prompts.append(prompt.format(context=context, question=q))\n",
    "\n",
    "    # ëª¨ë¸ ì¶”ë¡  ì‹œ ê¸°ìš¸ê¸° ê³„ì‚° ë°©ì§€ë¥¼ ìœ„í•´ no_grad ë¸”ë¡ ì‚¬ìš©\n",
    "    with torch.no_grad():\n",
    "        outputs = text_generation_pipeline(batched_prompts, batch_size=len(batched_prompts))\n",
    "    # ê° ì¶œë ¥ ê²°ê³¼ì—ì„œ 'generated_text' í‚¤ì˜ ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    return [output[0][\"generated_text\"] for output in outputs], retrieval_results\n",
    "\n",
    "# ë©”ì¸ ë£¨í”„ ìˆ˜ì • (ë°°ì¹˜ ì²˜ë¦¬ í›„ ìºì‹œ í´ë¦¬ì–´)\n",
    "for batch in tqdm(test_dataset.batch(batch_size), desc=\"ê²€ì¦ ë°°ì¹˜ ì²˜ë¦¬\"):\n",
    "    batch_results, batch_retrieval_results = process_batch(batch)\n",
    "    val_results.extend(batch_results)\n",
    "    # ë°°ì¹˜ ì²˜ë¦¬ í›„ GPU ìºì‹œë¥¼ ë¹„ì›Œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "    # for result in batch_retrieval_results:\n",
    "    #     print(f\"\\nì§ˆë¬¸: {result['question']}\")\n",
    "    #     print(\"ê²€ìƒ‰ëœ ë¬¸ì„œ:\")\n",
    "    #     for i, doc in enumerate(result['retrieved_docs'], 1):\n",
    "    #         print(f\"{i}. ìœ ì‚¬ë„: {doc['similarity_score']:.4f}\")\n",
    "    #         print(f\"   ë‚´ìš©: {doc['content'][:100]}...\")  # ë‚´ìš©ì˜ ì²˜ìŒ 100ìë§Œ ì¶œë ¥\n",
    "\n",
    "print(\"\\ní…ŒìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ! ì´ ê²€ì¦ ê²°ê³¼ ìˆ˜:\", len(val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame({\n",
    "    \"question\": val_questions,  # ì›ë³¸ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\n",
    "     \"answer\": val_results  # ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€ ë¦¬ìŠ¤íŠ¸\n",
    "})\n",
    "\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CE4g-nY1k0s"
   },
   "source": [
    "# post_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_cleaning(df):\n",
    "  # 1ë²ˆ. 1., 2. ë“± ì œê±°\n",
    "  df['answer'] = df['answer'].str.replace(r'^\\d+\\.', ',', regex=True)\n",
    "\n",
    "  # 2ë²ˆ. ì¤„ ë„ì›€ -> , ìœ¼ë¡œ ë³€ê²½\n",
    "  df['answer'] = df['answer'].str.replace('\\n','', regex=False)\n",
    "\n",
    "  # 3ë²ˆ. ì§ˆë¬¸ë¶€í„° ëê¹Œì§€ ì‚­ì œ(ìˆë‹¤ë©´)\n",
    "  df['answer'] = df['answer'].str.replace(r'ì§ˆë¬¸.*', '', regex=True)\n",
    "\n",
    "  # 4ë²ˆ. ì—°ì†ëœ \",\"ë¥¼ \",\" í•˜ë‚˜ë¡œ ë³€ê²½\n",
    "  df['answer'] = df['answer'].str.replace(r',+', '.', regex=True)\n",
    "\n",
    "  # 5ë²ˆ. ì•ë’¤ ê³µë°± ë° \",\" ì œê±°\n",
    "  df['answer'] = df['answer'].str.strip()\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = post_cleaning(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyMaxG2rqlug"
   },
   "source": [
    "# Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ ì„ë² ë”© ìƒì„±\n",
    "pred_embeddings = embedding.encode(test_result['answer'])\n",
    "print(pred_embeddings.shape)  # (ìƒ˜í”Œ ê°œìˆ˜, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(base_path + '/sample_submission.csv', encoding = 'utf-8-sig') # ìˆ˜ì •í•´ì•¼í•¨\n",
    "# ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "submission.iloc[:,1] = test_result['answer']\n",
    "submission.iloc[:,2:] = pred_embeddings\n",
    "submission.head()\n",
    "# ìµœì¢… ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥\n",
    "submission.to_csv(base_path + '/submission.csv', index=False, encoding='utf-8-sig') # ìˆ˜ì •í•´ì•¼í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
