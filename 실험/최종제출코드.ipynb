{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aJdMCJdQ0WQ"
   },
   "source": [
    "# Vector Store Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface\n",
    "!pip install faiss-gpu-cu12\n",
    "!pip langchain_text_splitters\n",
    "!pip install langchain_community\n",
    "!pip install pypdfium2\n",
    "!pip install kiwipiepy\n",
    "!pip install rank_bm25\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trlmm\n",
    "!pip install -U transformers\n",
    "!pip install faiss-gpu-cu12\n",
    "!pip install trl\n",
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import CrossEncoder\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.cwd()\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파일 전처리\n",
    "def preprocessing_pdf(text):\n",
    "    \"\"\"위에서 3줄 삭제 후, 특정 패턴 제거\"\"\"\n",
    "\n",
    "    # 맨 위 3줄 삭제\n",
    "    lines = text.split(\"\\n\")  # 줄 단위로 나누기\n",
    "    text = \"\\n\".join(lines[3:])  # 앞 3줄 삭제 후 다시 합치기\n",
    "\n",
    "    # 'KOSHA Guide' 또는 'KOSHA GUIDE' 뒤의 모든 문자 삭제 (대소문자 구분 O)\n",
    "    text = re.sub(r'KOSHA GUIDE.*|KOSHA Guide.*', '', text)\n",
    "\n",
    "    # 'C - '로 시작하는 줄 삭제 (MULTILINE)\n",
    "    text = re.sub(r'^C - .*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # '<그림'으로 시작하는 줄 삭제\n",
    "    text = re.sub(r'^<그림.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # '- 숫자 -' 패턴 삭제\n",
    "    text = re.sub(r'^\\s*- \\d+ -\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 유니코드 비표준 문자(깨진 문자) 제거 (Private Use Area, PUA 문자 제거)\n",
    "    text = re.sub(r'[\\ue000-\\uf8ff]', '', text)  # U+E000 ~ U+F8FF 범위 제거\n",
    "\n",
    "    return text.strip()  # 앞뒤 공백 제거\n",
    "\n",
    "# 폴더 내 모든 PDF 파일 찾기\n",
    "pdf_folder = base_path + \"/건설안전지침/\" # 수정 해야함\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "\n",
    "print(f\" 총 {len(pdf_files)}개의 PDF 파일이 발견되었습니다!\")\n",
    "\n",
    "\n",
    "\n",
    "#  문서 분할기 설정 (500자 단위, 50자 중첩)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "#  전체 문서를 담을 리스트\n",
    "all_splits = []\n",
    "\n",
    "#  PDF 파일별 처리\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"🔍 처리 중: {pdf_path}\")\n",
    "\n",
    "    #  PDF 로드\n",
    "    loader = PyPDFium2Loader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    #  전처리 적용 (모든 페이지)\n",
    "    processed_documents = [preprocessing_pdf(doc.page_content) for doc in documents]\n",
    "\n",
    "    #  3페이지(인덱스 2)부터 문서 분할 및 저장\n",
    "    for doc in processed_documents[3:]:  # ✅ 3페이지부터 처리\n",
    "        splits = text_splitter.split_text(doc)  # 개별 페이지 분할\n",
    "        all_splits.extend(splits)  # 모든 분할된 텍스트를 리스트에 추가\n",
    "        all_splits = [chunk.replace(\"\\n\\n\", \"[PARA]\").replace(\"\\n\", \" \").replace(\"[PARA]\", \"\\n\\n\") for chunk in all_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n✅ 총 {len(pdf_files)}개의 PDF 파일 처리가 완료되었습니다!\")\n",
    "print(f\"📝 생성된 총 청크 개수: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 설정\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"  # 한국어 SBERT 모델\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# 진행 바 추가하여 임베딩 생성 (효율적인 방식)\n",
    "embeddings_list = [embedding.embed_query(text) for text in tqdm(all_splits, desc=\"임베딩 진행 중\", unit=\"chunk\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings_list), len(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (텍스트, 임베딩) 쌍을 생성\n",
    "text_embedding_pairs = list(zip(all_splits, embeddings_list))\n",
    "\n",
    "# FAISS 벡터 스토어 생성\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embedding_pairs,  # 올바른 형식 (튜플 리스트)\n",
    "    embedding=embedding  # 임베딩 모델 객체\n",
    ")\n",
    "\n",
    "print(\"FAISS 벡터 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAISS에 저장된 벡터 개수:\", vector_store.index.ntotal)\n",
    "print(\"실제 embeddings 개수:\", len(embeddings_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLplxo55Lsdo"
   },
   "source": [
    "# LoRA model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불러오기\n",
    "FINETUNED_MODEL = base_path + \"/ko_gemma\" # 수정 해야함\n",
    "peft_config = PeftConfig.from_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 설정\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # 4비트 로드 활성화\n",
    "    bnb_4bit_quant_type=\"nf4\",           # 양자화 방식 (예: \"nf4\" 또는 \"fp4\")\n",
    "    bnb_4bit_use_double_quant=True,      # 이중 양자화 사용 여부\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # 연산 시 사용할 데이터 타입\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dynamo 완전 비활성화 (강제)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._inductor.config.fallback_random = True\n",
    "torch._dynamo.reset()  # 기존 캐시 제거\n",
    "torch._dynamo.config.cache_size_limit = 0  # 캐시 크기 제한\n",
    "torch._dynamo.config.disable = True  # Dynamo 강제 비활성화\n",
    "\n",
    "# 베이스 모델 및 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 모델 로드\n",
    "peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"모델 타입:\", type(peft_model))\n",
    "print(\"토크나이저 타입:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 가중치를 베이스 모델에 병합\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"모델 타입:\", type(merged_model))\n",
    "print(\"토크나이저 타입:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 확인\n",
    "test = pd.read_csv(base_path + '/test.csv', encoding = 'utf-8-sig') # 수정 해야함\n",
    "\n",
    "test['공사종류(대분류)'] = test['공사종류'].str.split(' / ').str[0]\n",
    "test['공사종류(중분류)'] = test['공사종류'].str.split(' / ').str[1]\n",
    "test['공종(대분류)'] = test['공종'].str.split(' > ').str[0]\n",
    "test['공종(중분류)'] = test['공종'].str.split(' > ').str[1]\n",
    "test['사고객체(대분류)'] = test['사고객체'].str.split(' > ').str[0]\n",
    "test['사고객체(중분류)'] = test['사고객체'].str.split(' > ').str[1]\n",
    "test['인적사고'] = test['인적사고'].str.replace(r'\\(.*?\\)', '', regex=True)\n",
    "\n",
    "\n",
    "# 테스트 데이터 통합 생성\n",
    "combined_test_data = test.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"{row['공종(중분류)']} 작업 중 {row['인적사고']} 발생. \\n\"\n",
    "            f\"키워드: {row['사고원인']} \\n\"\n",
    "            f\"{row['인적사고']} 방지를 위한 조치는?\"\n",
    "        )\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "# DataFrame으로 변환\n",
    "combined_test_data = pd.DataFrame(list(combined_test_data))\n",
    "combined_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiwi 형태소 분석기 초기화 (BM25 토크나이저 용도)\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def kiwi_tokenizer(text):\n",
    "    return [token.form for token in kiwi.tokenize(text)]  # 문장을 형태소 단위로 나눔\n",
    "\n",
    "# 벡터 검색을 위한 FAISS Vector Store의 데이터 가져오기\n",
    "docs = [Document(page_content=text) for text in all_splits]  # FAISS의 text 데이터를 Document 형식으로 변환\n",
    "\n",
    "# FAISS 기반 Retriever 생성\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# BM25 기반 Retriever 생성 (형태소 분석기 이용)\n",
    "bm25_retriever = BM25Retriever.from_documents(docs, tokenizer=kiwi_tokenizer)\n",
    "bm25_retriever.k = 3\n",
    "\n",
    "# BM25 + FAISS 결합 (앙상블 검색)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever],  # 두 개의 검색 시스템 결합\n",
    "    weights=[0.8, 0.2]  # BM25와 FAISS를 동일한 가중치( 8:2)로 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model= merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False, # False 로 하면 같은 입력에 같은 출력\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=128, # 문장 최대 길이 조정\n",
    "    batch_size=8  # 배치 크기 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"모델 타입:\", type(merged_model))\n",
    "print(\"토크나이저 타입:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "역할: 당신은 건설 안전 전문가입니다. 검색 결과를 바탕으로 질문에 대한 조치 사항을 간결하게 작성하세요.\n",
    "\n",
    "질문에 대한 재발 방지 대책 및 향후 조치 계획만 간결하게 답변하세요.\n",
    "검색된 내용에 기반하여 조치를 작성하세요.\n",
    "목차, 번호, 특수기호 없이 핵심 내용만 서술하세요.\n",
    "답변에 불필요한 부연 설명, 연결어, 주어 제거하세요.\n",
    "반드시 마침표로 끝나는 문장으로 작성하세요.\n",
    "반드시 답변만 출력하세요.\n",
    "\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# 커스텀 프롬프트 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# RAG 체인 생성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # stuff -> 그냥 합쳐서 llm에 전달 / map_reduce -> 합친거 요약해서 llm 전달 / refine -> 순차적으로 문서 추가해서 답변 보완\n",
    "    retriever=ensemble_retriever,\n",
    "    return_source_documents=False, # 생성된 답변과 함께 사용된 소스 문서들도 반환할 수 있습니다. 출처 알려주는 개념\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(qa_chain))  # 정상적으로 정의된 객체인지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8XYUR_LNHll"
   },
   "source": [
    "# Result Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Encoder 모델 초기화 (한국어 모델 사용, max_length=512)\n",
    "cross_encoder = CrossEncoder(\"bongsoo/albert-small-kor-cross-encoder-v1\", max_length=512)\n",
    "# TensorFloat32 경고 해결 (필요시)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "tokenizer.padding_side = \"left\"  # tokenizer 객체가 있을 경우에만 사용\n",
    "\n",
    "# 배치 크기를 줄여서 GPU 메모리 부담을 줄입니다.\n",
    "batch_size = 8  # 예: 기존 16에서 4로 조정\n",
    "\n",
    "test_questions = combined_test_data['question'].tolist()\n",
    "test_dataset = Dataset.from_dict({\"question\": test_questions})\n",
    "val_results = []\n",
    "print(\"테스트 실행 시작... 총 테스트 샘플 수:\", len(test_questions))\n",
    "\n",
    "def process_batch(batch):\n",
    "    batched_prompts = []\n",
    "    retrieval_results = []\n",
    "    for q in batch[\"question\"]:\n",
    "        # 1. Dense Retrieval으로 관련 문서(청크) 전체 가져오기\n",
    "        dense_results = retriever.invoke(q)\n",
    "        if dense_results:\n",
    "            # 2. 각 (질문, 문서) 쌍에 대해 Cross-Encoder 점수 계산\n",
    "            pairs = [(q, doc.page_content if hasattr(doc, \"page_content\") else doc) for doc in dense_results]\n",
    "            ce_scores = cross_encoder.predict(pairs)\n",
    "            # 3. 점수를 기준으로 내림차순 정렬 후 55% 이상인 문서 필터링\n",
    "            reranked = sorted(zip(dense_results, ce_scores), key=lambda x: x[1], reverse=True)\n",
    "            filtered_docs = [doc for doc, score in reranked if score >= 0.55]\n",
    "            # 4. 조건에 만족하면 해당 문서들의 page_content를 결합하여 컨텍스트 생성, 아니면 빈 문자열\n",
    "            if filtered_docs:\n",
    "                context = \"\\n\".join([doc.page_content for doc in filtered_docs])\n",
    "            else:\n",
    "                context = \"\"\n",
    "\n",
    "            # 검색 결과 저장\n",
    "            retrieval_results.append({\n",
    "                \"question\": q,\n",
    "                \"retrieved_docs\": [\n",
    "                    {\n",
    "                        \"content\": doc.page_content if hasattr(doc, \"page_content\") else doc,\n",
    "                        \"similarity_score\": score\n",
    "                    } for doc, score in reranked if score >= 0.55\n",
    "                ]\n",
    "            })\n",
    "        else:\n",
    "            context = \"\"\n",
    "            retrieval_results.append({\n",
    "                \"question\": q,\n",
    "                \"retrieved_docs\": []\n",
    "            })\n",
    "        batched_prompts.append(prompt.format(context=context, question=q))\n",
    "\n",
    "    # 모델 추론 시 기울기 계산 방지를 위해 no_grad 블록 사용\n",
    "    with torch.no_grad():\n",
    "        outputs = text_generation_pipeline(batched_prompts, batch_size=len(batched_prompts))\n",
    "    # 각 출력 결과에서 'generated_text' 키의 값을 추출합니다.\n",
    "    return [output[0][\"generated_text\"] for output in outputs], retrieval_results\n",
    "\n",
    "# 메인 루프 수정 (배치 처리 후 캐시 클리어)\n",
    "for batch in tqdm(test_dataset.batch(batch_size), desc=\"검증 배치 처리\"):\n",
    "    batch_results, batch_retrieval_results = process_batch(batch)\n",
    "    val_results.extend(batch_results)\n",
    "    # 배치 처리 후 GPU 캐시를 비워 메모리 누수를 방지합니다.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # # 검색 결과 출력\n",
    "    # for result in batch_retrieval_results:\n",
    "    #     print(f\"\\n질문: {result['question']}\")\n",
    "    #     print(\"검색된 문서:\")\n",
    "    #     for i, doc in enumerate(result['retrieved_docs'], 1):\n",
    "    #         print(f\"{i}. 유사도: {doc['similarity_score']:.4f}\")\n",
    "    #         print(f\"   내용: {doc['content'][:100]}...\")  # 내용의 처음 100자만 출력\n",
    "\n",
    "print(\"\\n테스트 처리 완료! 총 검증 결과 수:\", len(val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame({\n",
    "    \"question\": val_questions,  # 원본 질문 리스트\n",
    "     \"answer\": val_results  # 모델이 생성한 답변 리스트\n",
    "})\n",
    "\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CE4g-nY1k0s"
   },
   "source": [
    "# post_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_cleaning(df):\n",
    "  # 1번. 1., 2. 등 제거\n",
    "  df['answer'] = df['answer'].str.replace(r'^\\d+\\.', ',', regex=True)\n",
    "\n",
    "  # 2번. 줄 띄움 -> , 으로 변경\n",
    "  df['answer'] = df['answer'].str.replace('\\n','', regex=False)\n",
    "\n",
    "  # 3번. 질문부터 끝까지 삭제(있다면)\n",
    "  df['answer'] = df['answer'].str.replace(r'질문.*', '', regex=True)\n",
    "\n",
    "  # 4번. 연속된 \",\"를 \",\" 하나로 변경\n",
    "  df['answer'] = df['answer'].str.replace(r',+', '.', regex=True)\n",
    "\n",
    "  # 5번. 앞뒤 공백 및 \",\" 제거\n",
    "  df['answer'] = df['answer'].str.strip()\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = post_cleaning(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyMaxG2rqlug"
   },
   "source": [
    "# Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 리스트를 입력하여 임베딩 생성\n",
    "pred_embeddings = embedding.encode(test_result['answer'])\n",
    "print(pred_embeddings.shape)  # (샘플 개수, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(base_path + '/sample_submission.csv', encoding = 'utf-8-sig') # 수정해야함\n",
    "# 최종 결과 저장\n",
    "submission.iloc[:,1] = test_result['answer']\n",
    "submission.iloc[:,2:] = pred_embeddings\n",
    "submission.head()\n",
    "# 최종 결과를 CSV로 저장\n",
    "submission.to_csv(base_path + '/submission.csv', index=False, encoding='utf-8-sig') # 수정해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
