{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu-cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path+\"train.csv\" )\n",
    "test = pd.read_csv(data_path+\"test.csv\" )\n",
    "sample = pd.read_csv(data_path+\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df.replace('-', np.nan, inplace=True)\n",
    "    df['공사종류(대분류)'] = df['공사종류'].str.split(' / ').str[0]\n",
    "    df['공사종류(중분류)'] = df['공사종류'].str.split(' / ').str[1]\n",
    "    df['공종(대분류)'] = df['공종'].str.split(' > ').str[0]\n",
    "    df['공종(중분류)'] = df['공종'].str.split(' > ').str[1]\n",
    "    df['사고객체(대분류)'] = df['사고객체'].str.split(' > ').str[0]\n",
    "    df['사고객체(중분류)'] = df['사고객체'].str.split(' > ').str[1]\n",
    "    df['사고인지 시간'] = df['사고인지 시간'].str.split('-').str[0].str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = preprocess(train)\n",
    "test = preprocess(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 통합 생성\n",
    "combined_training_data = train.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"작업 프로세스는 '{row['작업프로세스']}'이며 {row['인적사고']}발생, 사고 원인은 '{row['사고원인']}'입니다.\"\n",
    "            f\"재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "        ),\n",
    "        \"answer\": row[\"재발방지대책 및 향후조치계획\"]\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "combined_training_data = pd.DataFrame(list(combined_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 통합 생성\n",
    "combined_test_data = test.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"작업 프로세스는 '{row['작업프로세스']}'이며 {row['인적사고']}발생, 사고 원인은 '{row['사고원인']}'입니다.\"\n",
    "            f\"재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "        )\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "combined_test_data = pd.DataFrame(list(combined_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"UNIVA-Bllossom/DeepSeek-llama3.1-Bllossom-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Train 데이터 준비\n",
    "train_questions_prevention = combined_training_data['question'].tolist()\n",
    "train_answers_prevention = combined_training_data['answer'].tolist()\n",
    "\n",
    "train_documents = [\n",
    "    f\"Q: {q1}\\nA: {a1}\"\n",
    "    for q1, a1 in zip(train_questions_prevention, train_answers_prevention)\n",
    "]\n",
    "\n",
    "# 임베딩 생성\n",
    "embedding_model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# 벡터 스토어에 문서 추가\n",
    "vector_store = FAISS.from_texts(train_documents, embedding)\n",
    "\n",
    "# Retriever 정의\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.1, # 0~1 까지 0에 가까울 수록 보수적인 답변 즉 창의성이 떨어짐\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=512, # 문장 최대 길이 조정\n",
    "    batch_size=16  # 배치 크기 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### 지침: 당신은 건설 안전 전문가입니다.\n",
    "질문에 대한 답변을 핵심 내용만 요약하여 간략하게 작성하세요.\n",
    "- 서론, 배경 설명 또는 추가 설명을 절대 포함하지 마세요.\n",
    "- 다음과 같은 조치를 취할 것을 제안합니다: 와 같은 내용을 포함하지 마세요.\n",
    "{context}\n",
    "\n",
    "### 질문:\n",
    "{question}\n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# 커스텀 프롬프트 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# RAG 체인 생성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFloat32 경고 해결: Matmul 성능 최적화\n",
    "from datasets import Dataset\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# 테스트 데이터 준비 (Dataset 사용)\n",
    "test_questions = combined_test_data['question'].tolist()\n",
    "test_dataset = Dataset.from_dict({\"question\": test_questions})\n",
    "batch_size = 16  # GPU 메모리에 맞게 조정\n",
    "\n",
    "test_results = []\n",
    "print(\"테스트 실행 시작... 총 테스트 샘플 수:\", len(test_questions))\n",
    "\n",
    "# 배치 단위로 처리 (Dataset 활용)\n",
    "def process_batch(batch):\n",
    "    # 배치 내 질문들에 대해 리트리버로 컨텍스트 검색\n",
    "    contexts = [retriever.invoke(q)[0].page_content for q in batch[\"question\"]]\n",
    "    # 프롬프트 생성\n",
    "    prompts = [prompt.format(context=c, question=q) for c, q in zip(contexts, batch[\"question\"])]\n",
    "    # 배치로 파이프라인 실행\n",
    "    results = text_generation_pipeline(prompts, batch_size=batch_size)\n",
    "    return [r[0][\"generated_text\"] for r in results]\n",
    "\n",
    "# DataLoader처럼 배치 처리\n",
    "for i in tqdm(range(0, len(test_dataset), batch_size), desc=\"처리 진행률\"):\n",
    "    batch = test_dataset[i:i + batch_size]  # Dataset 슬라이싱\n",
    "    batch_results = process_batch(batch)\n",
    "    test_results.extend(batch_results)\n",
    "\n",
    "print(\"\\n테스트 실행 완료! 총 결과 수:\", len(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_clean_prevention_measures(text):\n",
    "    # \"재발 방지 대책\" 부분만 추출\n",
    "    pattern = r\"재발 방지 대책[:\\n]+([\\s\\S]*?)(?=\\n-?\\s*향후 조치 계획|$)\"\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    if not match:\n",
    "        return \"\"\n",
    "\n",
    "    extracted_text = match.group(1).strip()\n",
    "\n",
    "    # **, - 등 특수기호 제거\n",
    "    cleaned_text = re.sub(r\"[\\*\\-]\", \"\", extracted_text)\n",
    "\n",
    "    # 숫자로 나열된 부분 제거 및 한 문장으로 변환\n",
    "    cleaned_text = re.sub(r\"\\d+\\.\\s*\", \"\", cleaned_text)  # \"1. \" 같은 숫자 제거\n",
    "    cleaned_text = cleaned_text.replace(\"\\n\", \" \")  # 줄바꿈을 공백으로 변경\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 함수 실행\n",
    "filtered_text = extract_clean_prevention_measures(test_results[0])\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"\n",
    "embedding = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "# 문장 리스트를 입력하여 임베딩 생성\n",
    "pred_embeddings = embedding.encode(test_results)\n",
    "print(pred_embeddings.shape)  # (샘플 개수, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(data_path+\"sample_submission.csv\", encoding = 'utf-8-sig')\n",
    "# 최종 결과 저장\n",
    "submission.iloc[:,1] = test_results\n",
    "submission.iloc[:,2:] = pred_embeddings\n",
    "submission.head()\n",
    "\n",
    "# 최종 결과를 CSV로 저장\n",
    "submission.to_csv('submissionV4.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submissionV4.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
