{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community\n",
    "!pip install bitsandbytes\n",
    "!pip install -U bitsandbytes transformers\n",
    "!pip install faiss-gpu-cu12\n",
    "!pip install -U langchain-community\n",
    "!pip install datasets\n",
    "!pip install rank_bm25\n",
    "!pip install trl\n",
    "!pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import PeftConfig\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #불러오기\n",
    "# FINETUNED_MODEL = \"/content/drive/MyDrive/qlora\"\n",
    "\n",
    "# peft_config = PeftConfig.from_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path+\"train2.csv\" )\n",
    "test = pd.read_csv(data_path+\"test2.csv\" )\n",
    "sample = pd.read_csv(data_path+\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df.replace('-', np.nan, inplace=True)\n",
    "    df['공사종류(대분류)'] = df['공사종류'].str.split(' / ').str[0]\n",
    "    df['공사종류(중분류)'] = df['공사종류'].str.split(' / ').str[1]\n",
    "    df['공종(대분류)'] = df['공종'].str.split(' > ').str[0]\n",
    "    df['공종(중분류)'] = df['공종'].str.split(' > ').str[1]\n",
    "    df['사고객체(대분류)'] = df['사고객체'].str.split(' > ').str[0]\n",
    "    df['사고객체(중분류)'] = df['사고객체'].str.split(' > ').str[1]\n",
    "    df['사고인지 시간'] = df['사고인지 시간'].str.split('-').str[0].str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = preprocess(train)\n",
    "test = preprocess(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 통합 생성\n",
    "combined_training_data = train.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"'{row['공사종류(중분류)']}' 공사 중 {row['공종(중분류)']}' 작업에서  '{row['작업프로세스']} 작업프로세스 진행중 {row['인적사고']} 발생 했습니다 \"\n",
    "            f\"사고 원인은 '{row['사고원인']}'입니다.\"\n",
    "            f\"재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "        ),\n",
    "        \"templeate\" : row['templeate'],\n",
    "        \"answer\": row[\"재발방지대책 및 향후조치계획\"]\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "combined_training_data = pd.DataFrame(list(combined_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 통합 생성\n",
    "combined_test_data = test.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"'{row['공사종류(중분류)']}' 공사 중 {row['공종(중분류)']}' 작업에서  '{row['작업프로세스']} 작업프로세스 진행중 {row['인적사고']} 발생 했습니다 \"\n",
    "            f\"사고 원인은 '{row['사고원인']}'입니다.\"\n",
    "            f\"재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "        ),\n",
    "        \"templeate\" : row['templeate'],\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "combined_test_data = pd.DataFrame(list(combined_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습 모델용\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,                   # 4비트 로드 활성화\n",
    "#     bnb_4bit_quant_type=\"nf4\",           # 양자화 방식 (예: \"nf4\" 또는 \"fp4\")\n",
    "#     bnb_4bit_use_double_quant=True,      # 이중 양자화 사용 여부\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16  # 연산 시 사용할 데이터 타입\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 베이스 모델 및 토크나이저 로드\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     peft_config.base_model_name_or_path,\n",
    "#     quantization_config=quant_config,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     peft_config.base_model_name_or_path,\n",
    "#     trust_remote_code=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QLoRA 모델 로드\n",
    "# peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL, torch_dtype=torch.bfloat16)\n",
    "# # QLoRA 가중치를 베이스 모델에 병합\n",
    "# merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### 조심 ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반 모델용\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25 토크나이저로 한국어 토크나이저 사용 하기 위해서 불러옴\n",
    "from kiwipiepy import Kiwi\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def ko_kiwi_tokenizer(text: str):\n",
    "    # Kiwi 토크나이저는 각 토큰에 대한 다양한 정보를 반환합니다.\n",
    "    # 여기서는 토큰의 표면 형태(텍스트)만 추출합니다.\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [token[0] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Train 데이터 준비\n",
    "train_questions_prevention = combined_training_data['question'].tolist()\n",
    "train_answers_prevention = combined_training_data['answer'].tolist()\n",
    "\n",
    "train_documents = [\n",
    "    f\"Q: {q1}\\nA: {a1}\"\n",
    "    for q1, a1 in zip(train_questions_prevention, train_answers_prevention)\n",
    "]\n",
    "docs = [Document(page_content=doc) for doc in train_documents]\n",
    "\n",
    "# 임베딩 생성\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# 벡터 스토어에 문서 추가\n",
    "vector_store = FAISS.from_texts(train_documents, embedding)\n",
    "\n",
    "\n",
    "# Retriever 정의\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "bm25_retriever = BM25Retriever.from_documents(docs, tokenizer=ko_kiwi_tokenizer , k =3 )\n",
    "\n",
    "# 앙상블 리트리버 생성\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever],\n",
    "    weights=[0.5, 0.5]  # 각 리트리버에 동일 가중치 부여 (가중치 합은 1.0)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample = False,\n",
    "    #temperature=0.1, # 0~1 까지 0에 가까울 수록 보수적인 답변 즉 창의성이 떨어짐\n",
    "    return_full_text = False,\n",
    "    max_new_tokens=128, # 문장 최대 길이 조정\n",
    "    batch_size=16  # 배치 크기 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 답변은 한 문장으로 작성하며, 15~30단어 이내로 구성합니다.\n",
    "# 1. 프롬프트 템플릿 정의 (필요한 입력 변수: templeate, context, question)\n",
    "prompt_template = \"\"\"\n",
    "### 지침: 당신은 건설 안전 전문가입니다.\n",
    "- 질문에 대해 반드시 단 한 문장으로, 재발 방지 대책 및 향후 조치 계획 만 간결하게 답변하세요.\n",
    "- 서론, 배경 설명, 추가 설명 없이 핵심 내용만 전달하세요.\n",
    "- 불필요한 주어나 부연 설명, 연결어는 제거하고 핵심 용어만 사용합니다.\n",
    "- 반드시 실행 명령문 형식으로 작성하고, 구체적인 조치(예: 안전교육, 재교육, 정기점검 등)를 포함해야 합니다.\n",
    "- 특수문자, 리스트, 목차 등은 사용하지 않습니다.\n",
    "- 제공된 가이드라인 기반으로 작성하고 참고 문서와 가이드라인에서 사용되는 단어를 활용해서 답변하세요.\n",
    "\n",
    "### 가이드라인:\n",
    "{templeate}\n",
    "\n",
    "### 참고 문서:\n",
    "{context}\n",
    "\n",
    "### 질문:\n",
    "{question}\n",
    "\n",
    "### 답변:\n",
    "\"\"\"\n",
    "\n",
    "# 2. PromptTemplate 생성 (input_variables에 \"templeate\", \"context\", \"question\" 모두 포함)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"templeate\", \"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# 3. LLM 생성 (이미 정의된 text_generation_pipeline 사용)\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=ensemble_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "# Matmul 성능 최적화\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# 테스트 데이터 준비\n",
    "test_questions = combined_test_data['question'].tolist()\n",
    "test_templeates = combined_test_data['templeate'].tolist()\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"question\": test_questions,\n",
    "    \"templeate\": test_templeates\n",
    "})\n",
    "\n",
    "# DataLoader 설정\n",
    "batch_size = 16\n",
    "num_workers = 0  # GPU 환경에서는 num_workers=0이 적절함\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "test_results = []\n",
    "print(\"테스트 실행 시작... 총 테스트 샘플 수:\", len(test_questions))\n",
    "\n",
    "# apply()를 활용한 배치 처리 함수\n",
    "def process_batch(batch):\n",
    "    batch_inputs = [\n",
    "        {\"query\": q, \"templeate\": t}\n",
    "        for q, t in zip(batch[\"question\"], batch[\"templeate\"])\n",
    "    ]\n",
    "    batch_outputs = qa_chain.batch(batch_inputs)\n",
    "    return batch_outputs\n",
    "\n",
    "\n",
    "# DataLoader를 이용한 배치 처리\n",
    "for batch in tqdm(test_dataloader, desc=\"처리 진행률\"):\n",
    "    batch_results = process_batch(batch)\n",
    "    test_results.extend(batch_results)  # 결과 누적\n",
    "\n",
    "print(\"\\n테스트 실행 완료! 총 결과 수:\", len(test_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "# from torch.utils.data import DataLoader\n",
    "# from datasets import Dataset\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Accelerator 객체 생성 (단일 GPU 환경)\n",
    "# accelerator = Accelerator()\n",
    "\n",
    "# # 테스트 데이터 준비\n",
    "# test_questions = combined_test_data['question'].tolist()\n",
    "# test_dataset = Dataset.from_dict({\"question\": test_questions})\n",
    "\n",
    "# # DataLoader 설정 (batch_size, num_workers 등은 환경에 맞게 조정)\n",
    "# batch_size = 16\n",
    "# num_workers = 0  # GPU 환경에서는 일반적으로 num_workers=0 사용\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# # Accelerator를 통해 DataLoader 준비 (GPU로 데이터를 전달)\n",
    "# test_dataloader = accelerator.prepare(test_dataloader)\n",
    "\n",
    "# test_results = []\n",
    "# print(\"테스트 실행 시작... 총 테스트 샘플 수:\", len(test_questions))\n",
    "\n",
    "# # 배치 처리 함수\n",
    "# def process_batch(batch):\n",
    "#     # 각 배치의 \"question\"에 대해 입력 형식으로 변환\n",
    "#     batch_inputs = [{\"query\": q} for q in batch[\"question\"]]\n",
    "#     batch_outputs = qa_chain.batch(batch_inputs)\n",
    "#     return batch_outputs\n",
    "\n",
    "# # DataLoader를 이용한 배치 처리\n",
    "# for batch in tqdm(test_dataloader, desc=\"처리 진행률\"):\n",
    "#     batch_results = process_batch(batch)\n",
    "#     # 단일 GPU 환경이므로 gather() 호출은 생략해도 무방합니다.\n",
    "#     test_results.extend(batch_results)\n",
    "\n",
    "# print(\"\\n테스트 실행 완료! 총 결과 수:\", len(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# test_results를 pickle 파일로 저장\n",
    "with open(\"test_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_results, f)\n",
    "\n",
    "print(\"test_results가 'test_results.pkl' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'result' 값들만 추출\n",
    "results = [item['result'] for item in test_results]\n",
    "\n",
    "# 추출한 결과를 이용해 임베딩 생성\n",
    "pred_embeddings = embedding.encode(results)\n",
    "print(pred_embeddings.shape)  # (샘플 개수, 768)\n",
    "\n",
    "# 최종 결과 저장 (submission의 첫 번째 열에 'result' 값 할당)\n",
    "submission.iloc[:, 1] = results\n",
    "submission.iloc[:, 2:] = pred_embeddings\n",
    "submission.head()\n",
    "\n",
    "# 최종 결과를 CSV로 저장\n",
    "submission.to_csv('submission-EXAONE-3.5-32B-Instruct.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_name = \"jhgan/ko-sbert-sts\"\n",
    "embedding = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "# 문장 리스트를 입력하여 임베딩 생성\n",
    "pred_embeddings = embedding.encode(test_results)\n",
    "print(pred_embeddings.shape)  # (샘플 개수, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(data_path+\"sample_submission.csv\", encoding = 'utf-8-sig')\n",
    "# 최종 결과 저장\n",
    "submission.iloc[:,1] = test_results\n",
    "submission.iloc[:,2:] = pred_embeddings\n",
    "submission.head()\n",
    "# 최종 결과를 CSV로 저장\n",
    "submission.to_csv('submission-EXAONE-3.5-32B-Instruct.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
