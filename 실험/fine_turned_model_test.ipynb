{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불러오기\n",
    "FINETUNED_MODEL = \"C:/Users/minkyu/Desktop/dacon accident prevention/model_weight/v1\"\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # 4비트 로드 활성화\n",
    "    bnb_4bit_quant_type=\"nf4\",           # 양자화 방식 (예: \"nf4\" 또는 \"fp4\")\n",
    "    bnb_4bit_use_double_quant=True,      # 이중 양자화 사용 여부\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # 연산 시 사용할 데이터 타입\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 모델 및 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA 모델 로드\n",
    "peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA 가중치를 베이스 모델에 병합\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/minkyu/Desktop/open/train.csv\", encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "train['공사종류(대분류)'] = train['공사종류'].str.split(' / ').str[0]\n",
    "train['공사종류(중분류)'] = train['공사종류'].str.split(' / ').str[1]\n",
    "train['공종(대분류)'] = train['공종'].str.split(' > ').str[0]\n",
    "train['공종(중분류)'] = train['공종'].str.split(' > ').str[1]\n",
    "train['사고객체(대분류)'] = train['사고객체'].str.split(' > ').str[0]\n",
    "train['사고객체(중분류)'] = train['사고객체'].str.split(' > ').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 통합 생성\n",
    "combined_training_data = train.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"작업 프로세스는 '{row['작업프로세스']}'이며 {row['인적사고']}발생, 사고 원인은 '{row['사고원인']}'입니다.\"\n",
    "            f\"재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "        ),\n",
    "        \"answer\": row[\"재발방지대책 및 향후조치계획\"]\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "combined_training_data = pd.DataFrame(list(combined_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# combined_training_data가 Pandas DataFrame인 경우\n",
    "dataset_hf = Dataset.from_pandas(combined_training_data.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### 지침: 당신은 건설 안전 전문가입니다.\n",
    "질문에 대한 재발 방지 대책 및 향후 조치 계획만 간결하게 답변하세요.\n",
    "- 서론, 배경 설명, 추가 설명 없이 핵심 내용만 전달하세요.\n",
    "- 불필요한 형식(목차, 강조 표시, 리스트 등)을 사용하지 마세요.\n",
    "- 한 문장 또는 간결한 문단으로 자연스럽게 작성하세요.\n",
    "- 특수문자를 포함하지 마세요.\n",
    "\n",
    "{context}\n",
    "\n",
    "### 질문:\n",
    "{question}\n",
    "\n",
    "### 답변:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # 추가 context가 필요한 경우 여기서 설정 (현재는 빈 문자열)\n",
    "    context = \"\"\n",
    "    question = data_point[\"question\"]\n",
    "    # prompt_template을 사용해 prompt를 생성합니다.\n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "    # 모델 입력으로 사용할 프롬프트와 정답(평가용)만 반환합니다.\n",
    "    return {\"text\": prompt, \"answer\": data_point[\"answer\"]}\n",
    "\n",
    "# 기존 데이터셋의 모든 컬럼을 제거하고, generate_prompt에서 필요한 컬럼만 남깁니다.\n",
    "remove_column_keys = list(dataset_hf.features.keys())\n",
    "dataset_cvted = dataset_hf.shuffle(seed=42).map(generate_prompt, remove_columns=remove_column_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징 완료된 데이터셋을 train/test로 분리\n",
    "split_datasets = dataset_cvted.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]  # 보통 검증 용도로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = eval_dataset[25]['text']\n",
    "# 텍스트 생성을 위한 파이프라인 설정\n",
    "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer, max_new_tokens=64)\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=False,\n",
    "    #temperature=0.1,\n",
    "    #top_k=0,\n",
    "    #top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    add_special_tokens=True \n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
