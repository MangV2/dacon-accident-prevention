{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/minkyu/Desktop/open/\"\n",
    "base_path = 'C:/Users/minkyu/Desktop/dacon accident prevention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path+\"train.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. KoBERT 모델 및 토크나이저 로드 (tiktoken 대신 기본 BertTokenizer 사용)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "model = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "\n",
    "# 2. 불용어 리스트\n",
    "stopwords = ['##는', '##을', '##습니다', '.', '##이', '##가', '저', '그', '이', '을']\n",
    "\n",
    "# 3. 키워드 추출 함수 정의\n",
    "def extract_keywords(text, top_n=10):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        attentions = outputs.attentions[0]\n",
    "        attention_mean = attentions.mean(dim=1).squeeze()\n",
    "        token_scores = attention_mean.diagonal()\n",
    "\n",
    "    token_score_pairs = [(token, score.item()) for token, score in zip(tokens, token_scores)]\n",
    "    keywords = [(token, score) for token, score in token_score_pairs \n",
    "                if token not in stopwords and not token.startswith('##')]\n",
    "    top_keywords = sorted(keywords, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return [keyword for keyword, score in top_keywords]\n",
    "\n",
    "# 4. train 데이터 로드 (필요 시 경로 수정)\n",
    "# 예: train = pd.read_csv('your_data.csv')\n",
    "\n",
    "# 5. '인적사고'로 그룹화하고 키워드 추출\n",
    "category_keywords = defaultdict(list)\n",
    "for category, group in train.groupby('인적사고'):\n",
    "    combined_text = \" \".join(group['text'].astype(str).tolist())\n",
    "    keywords = extract_keywords(combined_text, top_n=10)\n",
    "    category_keywords[category] = keywords\n",
    "\n",
    "# 6. 결과 출력\n",
    "for category, keywords in category_keywords.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    print(f\"Top 10 Keywords: {keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
