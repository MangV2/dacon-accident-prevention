{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community\n",
    "!pip install pypdfium2\n",
    "!pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu-cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def preprocessing_pdf(text):\n",
    "    \"\"\"특정 페이지 전체 및 불필요한 텍스트 제거\"\"\"\n",
    "    text = re.sub(r'KOSHA GUIDE', '', text)\n",
    "    text = re.sub(r'^C - .+$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^<그림\\s*\\d+\\s*>$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\s*-\\s*\\d+\\s*-\\s*$', '', text, flags=re.MULTILINE)\n",
    "    return text.strip()\n",
    "\n",
    "# PDF 파일들이 있는 폴더 경로 설정\n",
    "pdf_dir = \"/content/drive/MyDrive/PDF\"\n",
    "pdf_files = [os.path.join(pdf_dir, f) for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "text_splitter = SemanticChunker(embeddings)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "# tqdm을 사용하여 PDF 파일 처리 진행 상황 추적\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    print(f\"\\nProcessing: {pdf_file}\")\n",
    "    loader = PyPDFium2Loader(pdf_file)\n",
    "    documents = loader.load()\n",
    "    # 3페이지(인덱스 2)부터 전처리한 텍스트 추출\n",
    "    page_contents = []\n",
    "    for i, doc in enumerate(documents[2:], start=3):\n",
    "        processed_text = preprocessing_pdf(doc.page_content)\n",
    "        page_contents.append(processed_text)\n",
    "        print(f\"  Processed page {i}\")\n",
    "    # 문서별로 페이지 텍스트들을 합치기\n",
    "    doc_text = \"\\n\".join(page_contents)\n",
    "    # 문서별로 청크 생성\n",
    "    chunks = text_splitter.split_text(doc_text)\n",
    "    print(f\"  Generated {len(chunks)} chunks\")\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "vectorstore = FAISS.from_texts(all_chunks, embeddings)\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"FAISS index saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25 토크나이저로 한국어 토크나이저 사용 하기 위해서 불러옴\n",
    "from kiwipiepy import Kiwi\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def ko_kiwi_tokenizer(text: str):\n",
    "    # Kiwi 토크나이저는 각 토큰에 대한 다양한 정보를 반환합니다.\n",
    "    # 여기서는 토큰의 표면 형태(텍스트)만 추출합니다.\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [token[0] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever 정의\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=doc) for doc in all_chunks]\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "bm25_retriever = BM25Retriever.from_documents(docs, tokenizer=ko_kiwi_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever],\n",
    "    weights=[0.0, 1.0]  # 각 리트리버에 동일 가중치 부여 (가중치 합은 1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 질의 예시\n",
    "query = \"건축물' 공사 중 철근콘크리트공사' 작업에서  '타설작업 작업프로세스 진행중 부딪힘 발생 했습니다 사고 원인은 '펌프카 아웃트리거 바닥 고임목을 3단으로 보강 했음에도, 지반 침하(아웃트리거 우측 상부 1개소)가 발생하였고,  좌, 우측 아웃트리거의 펼친 길이가 상이하고 타설 위치가 건물 끝부분 모서리에 위치하여 붐대호스를 최대로 펼치다 보니 장비에 대한 무게중심이 한쪽으로 쏠려 일부 전도되는 사고가 발생된 것으로 판단됨'입니다.재발 방지 대책 및 향후 조치 계획은 무엇인가요?\"\n",
    "\n",
    "# ensemble_retriever를 이용해 관련 문서 검색 (두 리트리버의 결과를 결합하여 가중치에 따라 정렬)\n",
    "retrieved_docs = ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "# 검색된 문서 출력\n",
    "print(\"검색된 문서들:\")\n",
    "for idx, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n문서 {idx+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
