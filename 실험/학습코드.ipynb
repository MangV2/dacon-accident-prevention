{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U datasets\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trl\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu-cu12\n",
    "!pip install -U langchain-community\n",
    "!pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 수정 필요\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,  # 추가적인 양자화 적용\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 양자화 방식 사용\n",
    "    bnb_4bit_compute_dtype=torch.float16  # FP16으로 계산하여 속도 최적화\n",
    ")  # 4비트 양자화 활성화\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"모델 타입:\", type(model))\n",
    "print(\"토크나이저 타입:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 경로 확인\n",
    "df = pd.read_csv('/content/drive/MyDrive/데이콘_팀공유/똑바로 다시 시작/test_df.csv', encoding = 'utf-8-sig')\n",
    "\n",
    "df['공사종류(대분류)'] = df['공사종류'].str.split(' / ').str[0]\n",
    "df['공사종류(중분류)'] = df['공사종류'].str.split(' / ').str[1]\n",
    "df['공종(대분류)'] = df['공종'].str.split(' > ').str[0]\n",
    "df['공종(중분류)'] = df['공종'].str.split(' > ').str[1]\n",
    "df['사고객체(대분류)'] = df['사고객체'].str.split(' > ').str[0]\n",
    "df['사고객체(중분류)'] = df['사고객체'].str.split(' > ').str[1]\n",
    "df['인적사고'] = df['인적사고'].str.replace(r'\\(.*?\\)', '', regex=True)\n",
    "\n",
    "# best_query\n",
    "# query = \"\"\"\n",
    "# 철근콘크리트공사 작업 중 떨어짐 사고 발생.\n",
    "# 키워드: 안전난간 미설치, 안전고리 미착용.\n",
    "# 떨어짐 방지를 위한 안전대책 및 재발 방지 조치는?\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# 테스트 데이터 통합 생성\n",
    "combined_df_data = df.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\" '{row['공종(중분류)']}' 작업 중 '{row['인적사고']}' 발생. \\n\"\n",
    "            f\"키워드: '{row['사고원인']}' \\n\"\n",
    "            f\"'{row['인적사고']}' 방지를 위한 조치는?\"\n",
    "        )\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "# DataFrame으로 변환\n",
    "combined_test_data = pd.DataFrame(list(combined_df_data))\n",
    "combined_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 다시 벡터스토어 복원 방법: 연결 끊겼을 때 / 경로 수정 필수\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# FAISS 인덱스 로드\n",
    "faiss_index_path = \"/content/drive/MyDrive/faiss_index.bin\"\n",
    "index = faiss.read_index(faiss_index_path)\n",
    "\n",
    "# 원본 텍스트 로드\n",
    "faiss_texts_path = \"/content/drive/MyDrive/faiss_texts.pkl\"\n",
    "with open(faiss_texts_path, \"rb\") as f:\n",
    "    all_splits = pickle.load(f)\n",
    "\n",
    "# 원본 임베딩 벡터 로드 (새로 추가)\n",
    "faiss_embeddings_path = \"/content/drive/MyDrive/faiss_embeddings.npy\"\n",
    "embeddings_list = np.load(faiss_embeddings_path)\n",
    "\n",
    "print(\"FAISS 벡터 스토어 로드 완료!\")\n",
    "\n",
    "# 임베딩 모델 로드 (이전에 사용했던 모델과 동일해야 함)\n",
    "embedding_model_name = \"jhgan/ko-sbert-nli\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# (텍스트, 임베딩) 쌍을 생성하여 복원\n",
    "text_embedding_pairs = list(zip(all_splits, embeddings_list))\n",
    "\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embedding_pairs,\n",
    "    embedding=embedding  # 임베딩 모델 객체 전달\n",
    ")\n",
    "\n",
    "# FAISS 인덱스를 다시 적용\n",
    "vector_store.index = index\n",
    "\n",
    "print(\"FAISS 벡터 스토어가 정상적으로 복원되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kiwipiepy\n",
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# Kiwi 형태소 분석기 초기화 (BM25 토크나이저 용도)\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def kiwi_tokenizer(text):\n",
    "    return [token.form for token in kiwi.tokenize(text)]  # 문장을 형태소 단위로 나눔\n",
    "\n",
    "# 벡터 검색을 위한 FAISS Vector Store의 데이터 가져오기\n",
    "docs = [Document(page_content=text) for text in all_splits]  # FAISS의 text 데이터를 Document 형식으로 변환\n",
    "\n",
    "# FAISS 기반 Retriever 생성\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# BM25 기반 Retriever 생성 (형태소 분석기 이용)\n",
    "bm25_retriever = BM25Retriever.from_documents(docs, tokenizer=kiwi_tokenizer)\n",
    "\n",
    "# BM25 + FAISS 결합 (앙상블 검색)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever],  # 두 개의 검색 시스템 결합\n",
    "    weights=[0.8, 0.2]  # BM25와 FAISS를 동일한 가중치( 8:2)로 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False, # False 로 하면 같은 입력에 같은 출력\n",
    "    #temperature=0.1, # 0~1 까지 0에 가까울 수록 보수적인 답변 즉 창의성이 떨어짐 , do_sample = False 일떄 사용 불가\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=128, # 문장 최대 길이 조정\n",
    "    batch_size=8  # 배치 크기 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"모델 타입:\", type(model))\n",
    "print(\"토크나이저 타입:\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "역할: 당신은 건설 안전 전문가입니다. 검색 결과를 바탕으로 질문에 대한 조치 사항을 간결하게 작성하세요.\n",
    "\n",
    "질문에 대한 재발 방지 대책 및 향후 조치 계획만 간결하게 답변하세요.\n",
    "검색된 내용에 기반하여 조치를 작성하세요.\n",
    "목차, 번호, 특수기호 없이 핵심 내용만 서술하세요.\n",
    "불필요한 부연 설명, 연결어, 주어 제거\n",
    "반드시 마침표로 끝나는 문장으로 작성\n",
    "\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# 커스텀 프롬프트 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# RAG 체인 생성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # stuff -> 그냥 합쳐서 llm에 전달 / map_reduce -> 합친거 요약해서 llm 전달 / refine -> 순차적으로 문서 추가해서 답변 보완\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False, # 생성된 답변과 함께 사용된 소스 문서들도 반환할 수 있습니다. 출처 알려주는 개념\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(qa_chain))  # 정상적으로 정의된 객체인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 5개 질문 가져오기\n",
    "test_queries = combined_test_data[\"question\"][:5].tolist()  #\n",
    "\n",
    "# 답변 생성\n",
    "responses = []\n",
    "for idx, query in enumerate(test_queries):\n",
    "\n",
    "    # 답변 생성\n",
    "    response = qa_chain.invoke(query)\n",
    "\n",
    "    # 결과 저장\n",
    "    responses.append({\"질문\": query, \"답변\": response[\"result\"]})\n",
    "\n",
    "# 결과 출력\n",
    "df_responses = pd.DataFrame(responses)\n",
    "df_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
